{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to _pyspark_\n",
    "\n",
    "[Spark][spark], like Hadoop itself, is a framework for programming with an abstraction of the map-reduce paradigm. Its main data structure (RDD) allows better utilization of the memory of the nodes, and this made it very popular in recent years. Spark was originally part of the Hadoop ecosystem, however it was so useful, that eventually it was decided to make it available as a stand-alone framework. Spark is written in [Scala][scala], but it suports APIs for Java, R and of course Python.\n",
    "\n",
    "Spark is made of 5 building blocks:\n",
    "\n",
    "* Spark core - the fundamentals components of the language. It provides distributed task dispatching, scheduling, and basic I/O functionalities, exposed through an API centered on the RDD abstraction.\n",
    "* Spark SQL - tools for working with DataFrames. It provides an API for embedding SQL scripts, as well as connections with an ODBC/JDBC server.\n",
    "* Spark streaming - facilitates tasks witha a data stream. It ingests data in mini-batches and performs RDD transformations on those mini-batches of data.\n",
    "* Spark MLlib - distributed versions of various machine learning (ML) algorithms.\n",
    "* Spark GraphX - graph processing framework.\n",
    "\n",
    "In our course, we will explore 3 of the 5 - Spark core, Spark SQL and Spark MLlib, and we will do it using the Python API - **pyspark**.\n",
    "\n",
    "[spark]: https://en.wikipedia.org/wiki/Apache_Spark \"Apache Spark - Wikipedia\"\n",
    "[scala]: https://en.wikipedia.org/wiki/Scala_(programming_language) \"Scala - Wikipedia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, collections, itertools\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "local_dir = \"file:///{d}/\".format(d=os.getcwd())\n",
    "\n",
    "if 'sc' not in globals():\n",
    "    conf = SparkConf().setAppName('appName').setMaster('local')\n",
    "    sc = SparkContext(conf=conf)\n",
    "    spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting spark on your machine\n",
    "\n",
    "1. Install Java8: `brew cask install adoptopenjdk/openjdk/adoptopenjdk8`\n",
    "1. Install scala: `brew install scala`\n",
    "1. Install apache-spark: `brew install apache-spark`\n",
    "1. Set these enviroment variables to your python interpreter path: `PYSPARK_PYTHON` and `PYSPARK_DRIVER_PYTHON`\n",
    "1. Set `JAVA_HOME` to the path of your java installation `export JAVA_HOME=$(/usr/libexec/java_home -v 1.8)`\n",
    "1. Install pyspark: `python3 -m install pyspark`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD\n",
    "**Resilient Distributed Dataset (RDD)** is the main data object in Spark and it is an abstraction of the data parallelization. This means that we can work with a single RDD, where in fact its data, as well as its processing, may be distributed in the cluster.\n",
    "\n",
    "Data sharing is slow in MapReduce due to replication, serialization, and disk IO (Actually, most Hadoop applications spend more than 90% of the time doing HDFS read-write operations.). Recognizing this problem, RDDs support **in-memory** processing computation. This means, it stores the state of memory as an object across the jobs and the object is sharable between those jobs.\n",
    "\n",
    "Two technical comments:\n",
    "\n",
    "* RDDs are immutable, which has a great influence on the appearence of Spark code.\n",
    "* If the elements of an RDD are tuples (which is a Spark data type, equivalent to Python tuples of length 2), then each tuple is automatically recognized as a pair of a **key** and a **value**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations vs Actions\n",
    "RDD **transformations** are operations applied on RDDs to yield a new RDD. On the other hand, **actions** are operations applied on RDDs to yield a non-RDD result (number, string, list, etc.). \n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "* Transformations:\n",
    "    * _map(func)_ - Returns a new distributed dataset, formed by passing each element of the source through a function func.\n",
    "    * _flatMap(func)_ - Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item).\n",
    "    * _filter(func)_ - Returns a new dataset formed by selecting those elements of the source on which func returns true.\n",
    "    * _union(otherDataset)_ - Returns a new dataset that contains the union of the elements in the source dataset and the argument.\n",
    "    * _groupByKey()_ - When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable(V)) pairs.\n",
    "    * _reduceByKey(func)_ - When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V, V) â‡’ V.\n",
    "    * _sortByKey([ascending])_ - When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the Boolean ascending argument.\n",
    "* Actions:\n",
    "    * _reduce(func)_ - Aggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.\n",
    "    * _count()_ - Returns the number of elements in the dataset.\n",
    "    * _take(n)_ - Returns an array with the first n elements of the dataset. \n",
    "    * _saveAsTextFile(path)_ - Writes the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark calls _toString()_ on each element to convert it to a line of text in the file.\n",
    "\n",
    "Two technical comments:\n",
    "\n",
    "* In most cases one applies a chain of transformations which ends with an action. Each RDD in such dependency chain has a pointer (dependency) to its parent RDD. Spark is **lazy**, so nothing will be executed until an action will trigger the chain. Therefore, RDD transformation is not a set of data but is a step in a program (might be the only step) telling Spark how to get data and what to do with it.\n",
    "* Spark is written in Scala, which does not support some of the functionalities of Python. This is why the Python API offers some additional transformations which are not part of the core functionalities, but a wrapper of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1 - RDD fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dick', 'by', 'herman', 'melville', 'by']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sc\\\n",
    "    .textFile(\"melville-moby_dick.txt\")\\\n",
    "    .flatMap(lambda line: line.split())\\\n",
    "    .filter(lambda word: word.isalpha())\\\n",
    "    .map(lambda word: word.lower())\n",
    "words.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How may words are in the book? (words contain only letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171870"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many _unique_ words are in the book?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dick', <pyspark.resultiterable.ResultIterable at 0x10d54bf90>),\n",
       " ('by', <pyspark.resultiterable.ResultIterable at 0x10d54be90>),\n",
       " ('herman', <pyspark.resultiterable.ResultIterable at 0x10d568e50>),\n",
       " ('melville', <pyspark.resultiterable.ResultIterable at 0x10d54d810>),\n",
       " ('a', <pyspark.resultiterable.ResultIterable at 0x10d54bfd0>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = words.groupBy(lambda word: word)\n",
    "unique_words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13739"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the most common word in the book?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 14226), ('of', 6545), ('and', 6238), ('a', 4597), ('to', 4518), ('in', 4058), ('that', 2744), ('his', 2485), ('it', 1765), ('i', 1724)]\n"
     ]
    }
   ],
   "source": [
    "word_count = unique_words\\\n",
    "    .mapValues(lambda group: len(group))\\\n",
    "    .sortBy(lambda word_count: word_count[1], ascending=False)\n",
    "print (word_count.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the most common word in the book which is not a [stop-word][1]? (a file with the English stop-words is available in the folder)\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Stop_words \"Stop words - Wikipedia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 1), ('a', 1), ('aa', 1), ('aal', 1), ('aalii', 1), ('aam', 1), ('Aani', 1), ('aardvark', 1), ('aardwolf', 1), ('Aaron', 1)]\n"
     ]
    }
   ],
   "source": [
    "stop_words = sc\\\n",
    "    .textFile(\"english words.txt\")\\\n",
    "    .map(lambda word: (word, 1))\n",
    "print (stop_words.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_count_2 = word_count\\\n",
    "    .subtractByKey(stop_words)\\\n",
    "    .sortBy(lambda word_count: word_count[1], ascending=False)\n",
    "print (word_count_2.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "Read the file \"english words\" into an RDD and answer the following questions:\n",
    "1. How many words are listed in the file?\n",
    "1. What is the most common first letter?\n",
    "1. What is the longest word in the file?\n",
    "1. How many words include all 5 vowels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark and its related packages are constantly changing, and even the most basic scripts may become unusable from version to version. Threfore it is a good idea to be familiar with the documentation part, which is (trying to be) updated and helpful. Here are some relevant documentation links:\n",
    "\n",
    "* [Spark 2.0.2][spark]\n",
    "    * General concepts\n",
    "        * [Programing guide][pg]\n",
    "        * [Data structures][ds] - this includes explanations about DataFrames, DataSets and SQL\n",
    "    * Python API\n",
    "        * [pyspark package][pyspark] - this includes the [SparkConf][conf], [SparkContext][sc] and [RDD][rdd] classes\n",
    "        * [pyspark.sql module][sql] - this includes the [SparkSession][ss], [DataFrame][df], [Row][row] and [Column][col] classes\n",
    "\n",
    "[spark]: https://spark.apache.org/docs/2.0.2/index.html \"Spark 2.0.2\"\n",
    "[pg]: https://spark.apache.org/docs/2.0.2/programming-guide.html \"Spark programming guide\"\n",
    "[ds]: https://spark.apache.org/docs/2.0.2/sql-programming-guide.html \"Data structures programming guide\"\n",
    "[pyspark]: https://spark.apache.org/docs/2.0.2/api/python/index.html \"pyspark\"\n",
    "[conf]: https://spark.apache.org/docs/2.0.2/api/python/pyspark.html#pyspark.SparkConf \"SparkConf\"\n",
    "[sc]: https://spark.apache.org/docs/2.0.2/api/python/pyspark.html#pyspark.SparkContext \"SparkContext\"\n",
    "[rdd]: https://spark.apache.org/docs/2.0.2/api/python/pyspark.html#pyspark.RDD \"RDD\"\n",
    "[sql]: https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html \"pyspark.sql module\"\n",
    "[ss]: https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.SparkSession \"SparkSession\"\n",
    "[df]: https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame \"DataFrame\"\n",
    "[row]: https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.Row \"Row\"\n",
    "[col]: https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.Column \"Column\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "Introduction to PySpark",
  "notebookId": 2317684797154294
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
