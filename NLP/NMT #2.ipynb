{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CKQdeaqDVUDq"
   },
   "source": [
    "# NMT Workshop Excercise 2: Transliteration\n",
    "\n",
    "In this excercise we will train a seq2seq model to transliterate Hebrew text into Latin characters, without any prior knowledge of Hebrew.\n",
    "\n",
    "## Part 1: Hebrew Unicode\n",
    "\n",
    "For our purposes it will be useful to know a bit about how text in Hebrew is encoded in Python strings.\n",
    "\n",
    "Recall that in Python a string is made up of **characters** than can be accessed with square brackets. The length of the string is the number of characters it contains:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T10:35:42.338736Z",
     "start_time": "2019-08-08T10:35:42.333407Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2556,
     "status": "ok",
     "timestamp": 1565233773089,
     "user": {
      "displayName": "Morris Alper",
      "photoUrl": "https://lh3.googleusercontent.com/--_gcOdCIoAM/AAAAAAAAAAI/AAAAAAAAFCI/ar-HeAB3FNk/s64/photo.jpg",
      "userId": "15842932163458061285"
     },
     "user_tz": -180
    },
    "id": "SAC8qjCQl3OI",
    "outputId": "63640b51-723f-4d16-9338-44e09650884f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e o 5\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\"[1], \"hello\"[4], len(\"hello\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WK7rA8V6l5SE"
   },
   "source": [
    "In Python 3, a string is a sequence of **Unicode code points**, or unique numeric identifiers for each character. Python lets us see the Unicode code point for a character by using the built-in function *ord*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T10:35:47.732191Z",
     "start_time": "2019-08-08T10:35:47.726094Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2550,
     "status": "ok",
     "timestamp": 1565233773090,
     "user": {
      "displayName": "Morris Alper",
      "photoUrl": "https://lh3.googleusercontent.com/--_gcOdCIoAM/AAAAAAAAAAI/AAAAAAAAFCI/ar-HeAB3FNk/s64/photo.jpg",
      "userId": "15842932163458061285"
     },
     "user_tz": -180
    },
    "id": "_ybIlFDVnc1P",
    "outputId": "632859d7-1d9c-4e74-fd84-97a12cf64293"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unicode code points for characters in 'hello': 104 101 108 108 111\n"
     ]
    }
   ],
   "source": [
    "print(\"Unicode code points for characters in 'hello':\", *[ord(char) for char in \"hello\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UB3rxxdjnowo"
   },
   "source": [
    "**Questions**\n",
    "  1. What are the Unicode code points for each character in the word \"naivete\"? What about when it is written \"naïveté\"?\n",
    "  2. Use the built-in Python function *hex* to get the hexidecimal (base-16) values for these code points. What are they?\n",
    "  3. Use the [Show Unicode Character](http://qaz.wtf/u/show.cgi) tool to look at the Unicode characters in each of these two words. Where can we see the code point values? What about the names of the unicode characters?\n",
    "  4. What is the difference between the words \"naïveté\" and \"naïveté\"? What is the length of each as a Python string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T10:52:57.858947Z",
     "start_time": "2019-08-08T10:52:57.852007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unicode code points for characters in 'naiveté': 110 97 105 118 101 116 101 769\n"
     ]
    }
   ],
   "source": [
    "print(\"Unicode code points for characters in 'naiveté':\", *[ord(char) for char in \"naiveté\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T10:52:56.556682Z",
     "start_time": "2019-08-08T10:52:56.546120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unicode code points for characters in 'naïveté': 110 97 105 776 118 101 116 101 769\n"
     ]
    }
   ],
   "source": [
    "print(\"Unicode code points for characters in 'naïveté':\", *[ord(char) for char in \"naïveté\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T10:52:53.430403Z",
     "start_time": "2019-08-08T10:52:53.422755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hexidecimal (base-16) points for characters in 'naiveté': 0x6e 0x61 0x69 0x76 0x65 0x74 0x65 0x301\n"
     ]
    }
   ],
   "source": [
    "print(\"hexidecimal (base-16) points for characters in 'naiveté':\", *[hex(ord(char)) for char in \"naiveté\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T10:52:51.727860Z",
     "start_time": "2019-08-08T10:52:51.718022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hexidecimal (base-16) points for characters in 'naïveté': 0x6e 0x61 0x69 0x308 0x76 0x65 0x74 0x65 0x301\n"
     ]
    }
   ],
   "source": [
    "print(\"hexidecimal (base-16) points for characters in 'naïveté':\", *[hex(ord(char)) for char in \"naïveté\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\n",
    "\n",
    "Info for string \"naïveté\"\n",
    "\n",
    "110     006E     n     LATIN SMALL LETTER N\n",
    "\n",
    "97     0061     a     LATIN SMALL LETTER A\n",
    "\n",
    "105     0069     i     LATIN SMALL LETTER I\n",
    "\n",
    "776     0308     ̈     COMBINING DIAERESIS\n",
    "\n",
    "118     0076     v     LATIN SMALL LETTER V\n",
    "\n",
    "101     0065     e     LATIN SMALL LETTER E\n",
    "\n",
    "116     0074     t     LATIN SMALL LETTER T\n",
    "\n",
    "101     0065     e     LATIN SMALL LETTER E\n",
    "\n",
    "769     0301     ́     COMBINING ACUTE ACCENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Info for string \"naiveté\"\n",
    "\n",
    "110     006E     n     LATIN SMALL LETTER N\n",
    "\n",
    "97     0061     a     LATIN SMALL LETTER A\n",
    "\n",
    "105     0069     i     LATIN SMALL LETTER I\n",
    "\n",
    "118     0076     v     LATIN SMALL LETTER V\n",
    "\n",
    "101     0065     e     LATIN SMALL LETTER E\n",
    "\n",
    "116     0074     t     LATIN SMALL LETTER T\n",
    "\n",
    "101     0065     e     LATIN SMALL LETTER E\n",
    "\n",
    "769     0301     ́     COMBINING ACUTE ACCENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code point values are on the far left while the names are on the far right.\n",
    "\n",
    "the difference between both words is:\n",
    "\n",
    "776 0308 ̈ COMBINING DIAERESIS\n",
    "\n",
    "Which is the character  ̈ that is considered a separate character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T10:55:01.126896Z",
     "start_time": "2019-08-08T10:55:01.117727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of naiveté: 8\n"
     ]
    }
   ],
   "source": [
    "print(\"length of naiveté: {}\".format(len(\"naiveté\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T10:55:21.668392Z",
     "start_time": "2019-08-08T10:55:21.661410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of naïveté: 9\n"
     ]
    }
   ],
   "source": [
    "print(\"length of naïveté: {}\".format(len(\"naïveté\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WVMF2oPLpyS3"
   },
   "source": [
    "Hebrew words can be written either without vowels, or with vowel symbols called **nikkud**. Let's consider how these are represented in Python and in Unicode.\n",
    "\n",
    "**Questions:**\n",
    "  5. What are the first and last letters in the Python string for the Hebrew word בלשנות? What are their hexidecimal Unicode codepoints?\n",
    "  6. How many characters does the Hebrew string בַּלְשָׁנוּת have? Why this number?\n",
    "  7. What are the second and third characters of יִשְׂרָאֵל? What are their hexidecimal Unicode codepoints?\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T10:58:41.889639Z",
     "start_time": "2019-08-08T10:58:41.883737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first letter is: ב\n",
      "The last letter is: ת\n"
     ]
    }
   ],
   "source": [
    "my_string = \"בלשנות\"\n",
    "my_string[0]\n",
    "my_string[-1]\n",
    "print(\"The first letter is: {}\".format(my_string[0]))\n",
    "print(\"The last letter is: {}\".format(my_string[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T11:08:42.565685Z",
     "start_time": "2019-08-08T11:08:42.558697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hexidecimal (base-16) points for characters in בלשנות: \n",
      " 0x5d1 0x5dc 0x5e9 0x5e0 0x5d5 0x5ea\n"
     ]
    }
   ],
   "source": [
    "print(\"hexidecimal (base-16) points for characters in בלשנות: \\n\", *[hex(ord(char)) for char in my_string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T10:59:53.011464Z",
     "start_time": "2019-08-08T10:59:53.004482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length is: 12\n"
     ]
    }
   ],
   "source": [
    "my_string2 = \"בַּלְשָׁנוּת\"\n",
    "print(\"the length is: {}\".format(len(my_string2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count above 6 letters and 6 \"nikkuds\". Each nikkud is considered 1 character on top of the 6 characters that are the letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T11:13:18.711286Z",
     "start_time": "2019-08-08T11:13:18.704562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2nd character is: ִ  and the hexadecimal code is: 0x5b4\n",
      "The 2nd character is: ִ  and the hexadecimal code is: 0x5e9\n"
     ]
    }
   ],
   "source": [
    "my_string3 = \"יִשְׂרָאֵל\"\n",
    "print(\"The 2nd character is: {}  and the hexadecimal code is: {}\".format(my_string3[1], hex(ord(my_string3[1]))))\n",
    "print(\"The 2nd character is: {}  and the hexadecimal code is: {}\".format(my_string3[1], hex(ord(my_string3[2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J4RQLvApaGT4"
   },
   "source": [
    "## Part 2: Data processing\n",
    "\n",
    "We'll be using the data in the attached file *nikkud_seq2seq_data.csv* to train and test our model. This contains Hebrew words without nikkud (vowels), the words with nikkud, and their transliterations (pronunciation written in Latin characters), scraped from articles on the [Hebrew-language Wiktionary](https://he.wiktionary.org/wiki/%D7%A2%D7%9E%D7%95%D7%93_%D7%A8%D7%90%D7%A9%D7%99).\n",
    "\n",
    "**Questions:**\n",
    "  8. Load the data into a Pandas DataFrame variable *df*. How many entries does df contain? Looking at some sample entries, do the transliterations look correct?\n",
    "  9. See if you can find where the transliterations were taken from in Wiktionary. (follow the link above and search for the given words.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T21:03:48.785009Z",
     "start_time": "2019-08-16T21:03:35.596497Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T21:04:16.416059Z",
     "start_time": "2019-08-16T21:04:15.643963Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('nikkud_seq2seq_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T21:04:36.082445Z",
     "start_time": "2019-08-16T21:04:35.981089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15490, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T21:06:18.208631Z",
     "start_time": "2019-08-16T21:06:18.052860Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nikkud</th>\n",
       "      <th>transliteration</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3882</th>\n",
       "      <td>הֵד</td>\n",
       "      <td>hed</td>\n",
       "      <td>הד</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3412</th>\n",
       "      <td>חַי</td>\n",
       "      <td>khai</td>\n",
       "      <td>חי</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>הִצְטַנְּנוּת</td>\n",
       "      <td>hitztanenut</td>\n",
       "      <td>הצטננות</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>אֵין</td>\n",
       "      <td>ein</td>\n",
       "      <td>אין</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             nikkud transliteration     word\n",
       "3882            הֵד             hed       הד\n",
       "3412            חַי            khai       חי\n",
       "4347  הִצְטַנְּנוּת     hitztanenut  הצטננות\n",
       "680            אֵין             ein      אין"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes the transliterations look correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\n",
    "\n",
    "I looked and found them in the wiktionary.\n",
    "\n",
    "https://he.wiktionary.org/wiki/%D7%94%D7%A6%D7%98%D7%A0%D7%A0%D7%95%D7%AA#%D7%94%D6%B4%D7%A6%D6%B0%D7%98%D6%B7%D7%A0%D6%B0%D6%BC%D7%A0%D7%95%D6%BC%D7%AA\n",
    "\n",
    "https://he.wiktionary.org/wiki/%D7%90%D7%99%D7%9F\n",
    "\n",
    "https://he.wiktionary.org/wiki/%D7%97%D7%99\n",
    "\n",
    "https://he.wiktionary.org/wiki/%D7%94%D7%93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6cl1CB4ct0LK"
   },
   "source": [
    "Our model will be simpler if we pad all words to be the same length, and add start- and end-of-word characters. \n",
    "\n",
    "**Questions:**\n",
    "  10. Define variables *nikkud_maxlen* and *translit_maxlen* as the length of the longest word in the *nikkud* and *transliteration* columns, respectively. What are these lengths?\n",
    "  11. Define the function *pad_word* as shown in the comments below, to add start- and end-of-word characters to a word and pad it to a given length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T21:52:22.193326Z",
     "start_time": "2019-08-16T21:52:21.982480Z"
    }
   },
   "outputs": [],
   "source": [
    "#10.\n",
    "nikkud_maxlen = df['nikkud'].apply(len).max()\n",
    "translit_maxlen = df['transliteration'].apply(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T21:52:33.703715Z",
     "start_time": "2019-08-16T21:52:33.681100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nikkud_maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T21:53:02.665672Z",
     "start_time": "2019-08-16T21:53:02.649287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translit_maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T21:57:16.268959Z",
     "start_time": "2019-08-16T21:57:16.220608Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Mk0uqQT3OPcC"
   },
   "outputs": [],
   "source": [
    "# answer to question 11\n",
    "def pad_word(word, pad_length):       \n",
    "#### add code here so the function adds ^ to the beginning of the word, spaces  after the word, and $ at the end\n",
    "#### so that the output string is of length pad_length\n",
    "#### example: pad_word(\"hello\", 12) should return the string \"^hello     $\" which is of length 12\n",
    "    num_spaces = pad_length - len(word) - 2\n",
    "    return '^' + word + ' '*num_spaces + '$'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHU7IevT1Bp5"
   },
   "source": [
    "Now we define strings containing all characters used in our words, along with starting, padding, and ending tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T21:58:30.377095Z",
     "start_time": "2019-08-16T21:58:30.264076Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Y_qNYYAbK2ey"
   },
   "outputs": [],
   "source": [
    "nikkud_charset = '^$ ' + ''.join(sorted(set(''.join(df.nikkud))))\n",
    "translit_charset = '^$ ' + ''.join(sorted(set(''.join(df.transliteration))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q8MRAi9H1SsM"
   },
   "source": [
    "**Questions:**\n",
    "  12. How many characters are used in words with nikkud? In transliterations?\n",
    "  13. Try printing out these character sets? Do you see anything strange in the output? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T22:04:51.148455Z",
     "start_time": "2019-08-16T22:04:51.121606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nikkud_charset) - len('^$ ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T22:04:59.365361Z",
     "start_time": "2019-08-16T22:04:59.345513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(translit_charset) - len('^$ ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T22:02:03.896766Z",
     "start_time": "2019-08-16T22:02:03.874134Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'^$ \"\\'ְֱֲֳִֵֶַָֹֻּׁׂאבגדהוזחטיךכלםמןנסעףפץצקרשת'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nikkud_charset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the nikkud output that all the nikkuds were combined into the last letter of the hebrew alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T22:01:33.493498Z",
     "start_time": "2019-08-16T22:01:33.469780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'^$ \"\\'abcdefghijklmnopqrstuvwxyz'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translit_charset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5H9VUMPJ1sqL"
   },
   "source": [
    "Now let's define functions to produce sequence vectors from words with nikkud or transliterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T22:04:01.038702Z",
     "start_time": "2019-08-16T22:04:00.969557Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "klIXd7952tFj"
   },
   "outputs": [],
   "source": [
    "def nikkud2sequence(nikkud):\n",
    "  return [nikkud_charset.index(c) for c in pad_word(nikkud, nikkud_maxlen + 2)]\n",
    "def translit2sequence(translit):\n",
    "  return [translit_charset.index(c) for c in pad_word(translit, translit_maxlen + 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "onX7bnt53LJv"
   },
   "source": [
    "**Questions:**\n",
    "  14. What are the feature vectors for \"שָׁלוֹם\" and \"shalom\"? What do the numbers in the vectors mean?\n",
    "  15. Add code to the comment below, to define functions *nikkud2onehot* and *translit2onehot*. These should take in strings (either a Hebrew word with nikkud, or a transliteration) and return a matrix where each character is one-hot encoded. Hint: Use *keras.utils.to_categorical*, with attribute *num_classes = (number of characters in the character set)*.\n",
    "  16. If you implemented those functions correctly, nikkud2onehot('שָׁלוֹם').shape should equal (33, 46) and translit2onehot('shalom').shape should equal (27, 31). What do these dimensions mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T22:06:50.104556Z",
     "start_time": "2019-08-16T22:06:50.034244Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 44,\n",
       " 13,\n",
       " 17,\n",
       " 31,\n",
       " 24,\n",
       " 14,\n",
       " 32,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#14\n",
    "nikkud2sequence(\"שָׁלוֹם\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T22:07:26.826636Z",
     "start_time": "2019-08-16T22:07:26.763820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 23,\n",
       " 12,\n",
       " 5,\n",
       " 16,\n",
       " 19,\n",
       " 17,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translit2sequence(\"shalom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers in the vectors represent the indexes of the word's characters that are found in the respective character sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T22:12:55.221463Z",
     "start_time": "2019-08-16T22:11:50.185663Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T22:21:46.449569Z",
     "start_time": "2019-08-16T22:21:46.397416Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "LYH2ISZGLId_"
   },
   "outputs": [],
   "source": [
    "def nikkud2onehot(nikkud):\n",
    "    return to_categorical(nikkud2sequence(nikkud), num_classes=len(nikkud_charset))\n",
    "def translit2onehot(translit):\n",
    "    return to_categorical(translit2sequence(translit), num_classes=len(translit_charset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T22:21:48.439683Z",
     "start_time": "2019-08-16T22:21:48.404661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 46)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nikkud2onehot('שָׁלוֹם').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T22:22:09.117192Z",
     "start_time": "2019-08-16T22:22:09.096307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 31)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translit2onehot('shalom').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T22:22:38.222491Z",
     "start_time": "2019-08-16T22:22:38.189229Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nikkud2onehot('שָׁלוֹם')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape represents the size of each onehot encoded vector (the size of their respective character sets) and the number of characters in the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxq05Qv85osT"
   },
   "source": [
    "Now let's combine the matrixes for all the words together into tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T22:25:43.713839Z",
     "start_time": "2019-08-16T22:25:38.363684Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "lNnVXLE0LVQ2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([nikkud2onehot(nikkud) for nikkud in df.nikkud])\n",
    "Y = np.array([translit2onehot(translit) for translit in df.transliteration])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jnvo2gdD5xYS"
   },
   "source": [
    "Notice that the first dimension of each tensor is the sample size (number of words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T22:25:45.032114Z",
     "start_time": "2019-08-16T22:25:45.007727Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4322,
     "status": "ok",
     "timestamp": 1565233774987,
     "user": {
      "displayName": "Morris Alper",
      "photoUrl": "https://lh3.googleusercontent.com/--_gcOdCIoAM/AAAAAAAAAAI/AAAAAAAAFCI/ar-HeAB3FNk/s64/photo.jpg",
      "userId": "15842932163458061285"
     },
     "user_tz": -180
    },
    "id": "y9iKrVCj5uu8",
    "outputId": "767dd991-9965-4876-ec30-b5045cca518b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15490, 33, 46), (15490, 27, 31))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMpe8ST-8BSJ"
   },
   "source": [
    "In the seq2seq model that we will train, we will try to predict the next character in the transliteration from the characters already generated and from the given nikkud. Since Y contains the encoding for the characters in the transliteration, we want to shift it by one to represent the next character that needs to be predicted.  This is simple with the numpy function *np.roll*. We save this in the tensor Z which will be predicted by the model given X (nikkud) and Y (transliteration):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T22:28:22.582583Z",
     "start_time": "2019-08-16T22:28:22.088989Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "QoPnnICX7_uE"
   },
   "outputs": [],
   "source": [
    "Z = np.roll(Y, -1, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w6myoxPO61FF"
   },
   "source": [
    "## Part 3: Seq2seq with LSTMs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X0y3g2MB9T4a"
   },
   "source": [
    "We'll now build a seq2seq model with Keras to predict transliteration from nikkud. First let's build and train our model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T22:28:31.039205Z",
     "start_time": "2019-08-16T22:28:31.004346Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4295,
     "status": "ok",
     "timestamp": 1565233774990,
     "user": {
      "displayName": "Morris Alper",
      "photoUrl": "https://lh3.googleusercontent.com/--_gcOdCIoAM/AAAAAAAAAAI/AAAAAAAAFCI/ar-HeAB3FNk/s64/photo.jpg",
      "userId": "15842932163458061285"
     },
     "user_tz": -180
    },
    "id": "pHb4IkNH145m",
    "outputId": "78accd92-e7cf-41b5-bcf3-7ccbc3ded4d4"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T02:52:43.553662Z",
     "start_time": "2019-08-16T22:28:41.517405Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "XT2AoLZ7O58D"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0817 01:28:41.557801 140735958451072 deprecation_wrapper.py:119] From /Users/Yohan/Desktop/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0817 01:28:41.952869 140735958451072 deprecation_wrapper.py:119] From /Users/Yohan/Desktop/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0817 01:28:42.088677 140735958451072 deprecation_wrapper.py:119] From /Users/Yohan/Desktop/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0817 01:28:46.004592 140735958451072 deprecation_wrapper.py:119] From /Users/Yohan/Desktop/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0817 01:28:46.108327 140735958451072 deprecation_wrapper.py:119] From /Users/Yohan/Desktop/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0817 01:28:46.952708 140735958451072 deprecation.py:323] From /Users/Yohan/Desktop/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0817 01:28:54.410464 140735958451072 deprecation_wrapper.py:119] From /Users/Yohan/Desktop/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12392 samples, validate on 3098 samples\n",
      "Epoch 1/50\n",
      "12392/12392 [==============================] - 133s 11ms/step - loss: 1.2037 - val_loss: 0.9958\n",
      "Epoch 2/50\n",
      "12392/12392 [==============================] - 92s 7ms/step - loss: 0.9615 - val_loss: 0.8730\n",
      "Epoch 3/50\n",
      "12392/12392 [==============================] - 95s 8ms/step - loss: 0.8919 - val_loss: 0.9389\n",
      "Epoch 4/50\n",
      "12392/12392 [==============================] - 93s 8ms/step - loss: 0.8387 - val_loss: 0.8446\n",
      "Epoch 5/50\n",
      "12392/12392 [==============================] - 98s 8ms/step - loss: 0.7799 - val_loss: 0.7489\n",
      "Epoch 6/50\n",
      "12392/12392 [==============================] - 97s 8ms/step - loss: 0.7274 - val_loss: 0.7237\n",
      "Epoch 7/50\n",
      "12392/12392 [==============================] - 101s 8ms/step - loss: 0.6898 - val_loss: 0.6426\n",
      "Epoch 8/50\n",
      "12392/12392 [==============================] - 1379s 111ms/step - loss: 0.6617 - val_loss: 0.6224\n",
      "Epoch 9/50\n",
      "12392/12392 [==============================] - 148s 12ms/step - loss: 0.6354 - val_loss: 0.6112\n",
      "Epoch 10/50\n",
      "12392/12392 [==============================] - 100s 8ms/step - loss: 0.6199 - val_loss: 0.6559\n",
      "Epoch 11/50\n",
      "12392/12392 [==============================] - 95s 8ms/step - loss: 0.6028 - val_loss: 0.5954\n",
      "Epoch 12/50\n",
      "12392/12392 [==============================] - 97s 8ms/step - loss: 0.5918 - val_loss: 0.5917\n",
      "Epoch 13/50\n",
      "12392/12392 [==============================] - 112s 9ms/step - loss: 0.5786 - val_loss: 0.6193\n",
      "Epoch 14/50\n",
      "12392/12392 [==============================] - 112s 9ms/step - loss: 0.5693 - val_loss: 0.5783\n",
      "Epoch 15/50\n",
      "12392/12392 [==============================] - 97s 8ms/step - loss: 0.5574 - val_loss: 0.6306\n",
      "Epoch 16/50\n",
      "12392/12392 [==============================] - 100s 8ms/step - loss: 0.5495 - val_loss: 0.5536\n",
      "Epoch 17/50\n",
      "12392/12392 [==============================] - 100s 8ms/step - loss: 0.5392 - val_loss: 0.5648\n",
      "Epoch 18/50\n",
      "12392/12392 [==============================] - 100s 8ms/step - loss: 0.5269 - val_loss: 0.5803\n",
      "Epoch 19/50\n",
      "12392/12392 [==============================] - 114s 9ms/step - loss: 0.5143 - val_loss: 0.5323\n",
      "Epoch 20/50\n",
      "12392/12392 [==============================] - 98s 8ms/step - loss: 0.4995 - val_loss: 0.5004\n",
      "Epoch 21/50\n",
      "12392/12392 [==============================] - 98s 8ms/step - loss: 0.4826 - val_loss: 0.5076\n",
      "Epoch 22/50\n",
      "12392/12392 [==============================] - 99s 8ms/step - loss: 0.4728 - val_loss: 0.5014\n",
      "Epoch 23/50\n",
      "12392/12392 [==============================] - 103s 8ms/step - loss: 0.4602 - val_loss: 0.5310\n",
      "Epoch 24/50\n",
      "12392/12392 [==============================] - 100s 8ms/step - loss: 0.4501 - val_loss: 0.4832\n",
      "Epoch 25/50\n",
      "12392/12392 [==============================] - 117s 9ms/step - loss: 0.4406 - val_loss: 0.4877\n",
      "Epoch 26/50\n",
      "12392/12392 [==============================] - 2451s 198ms/step - loss: 0.4312 - val_loss: 0.4244\n",
      "Epoch 27/50\n",
      "12392/12392 [==============================] - 122s 10ms/step - loss: 0.4194 - val_loss: 0.4438\n",
      "Epoch 28/50\n",
      "12392/12392 [==============================] - 117s 9ms/step - loss: 0.4043 - val_loss: 0.4395\n",
      "Epoch 29/50\n",
      "12392/12392 [==============================] - 112s 9ms/step - loss: 0.3953 - val_loss: 0.4027\n",
      "Epoch 30/50\n",
      "12392/12392 [==============================] - 108s 9ms/step - loss: 0.3811 - val_loss: 0.4042\n",
      "Epoch 31/50\n",
      "12392/12392 [==============================] - 108s 9ms/step - loss: 0.3775 - val_loss: 0.4207\n",
      "Epoch 32/50\n",
      "12392/12392 [==============================] - 109s 9ms/step - loss: 0.3563 - val_loss: 0.4036\n",
      "Epoch 33/50\n",
      "12392/12392 [==============================] - 109s 9ms/step - loss: 0.3448 - val_loss: 0.3610\n",
      "Epoch 34/50\n",
      "12392/12392 [==============================] - 2497s 201ms/step - loss: 0.3317 - val_loss: 0.4276\n",
      "Epoch 35/50\n",
      "12392/12392 [==============================] - 2075s 167ms/step - loss: 0.3154 - val_loss: 0.3309\n",
      "Epoch 36/50\n",
      "12392/12392 [==============================] - 119s 10ms/step - loss: 0.3079 - val_loss: 0.3648\n",
      "Epoch 37/50\n",
      "12392/12392 [==============================] - 119s 10ms/step - loss: 0.2871 - val_loss: 0.3076\n",
      "Epoch 38/50\n",
      "12392/12392 [==============================] - 113s 9ms/step - loss: 0.2684 - val_loss: 0.5903\n",
      "Epoch 39/50\n",
      "12392/12392 [==============================] - 117s 9ms/step - loss: 0.2708 - val_loss: 0.2817\n",
      "Epoch 40/50\n",
      "12392/12392 [==============================] - 108s 9ms/step - loss: 0.2560 - val_loss: 0.2398\n",
      "Epoch 41/50\n",
      "12392/12392 [==============================] - 109s 9ms/step - loss: 0.2259 - val_loss: 0.2534\n",
      "Epoch 42/50\n",
      "12392/12392 [==============================] - 2587s 209ms/step - loss: 0.2207 - val_loss: 0.2157\n",
      "Epoch 43/50\n",
      "12392/12392 [==============================] - 116s 9ms/step - loss: 0.2111 - val_loss: 0.2393\n",
      "Epoch 44/50\n",
      "12392/12392 [==============================] - 117s 9ms/step - loss: 0.2031 - val_loss: 0.2156\n",
      "Epoch 45/50\n",
      "12392/12392 [==============================] - 108s 9ms/step - loss: 0.1918 - val_loss: 0.2116\n",
      "Epoch 46/50\n",
      "12392/12392 [==============================] - 107s 9ms/step - loss: 0.1807 - val_loss: 0.1861\n",
      "Epoch 47/50\n",
      "12392/12392 [==============================] - 105s 8ms/step - loss: 0.1745 - val_loss: 0.1951\n",
      "Epoch 48/50\n",
      "12392/12392 [==============================] - 104s 8ms/step - loss: 0.1612 - val_loss: 0.2078\n",
      "Epoch 49/50\n",
      "12392/12392 [==============================] - 109s 9ms/step - loss: 0.1609 - val_loss: 0.2346\n",
      "Epoch 50/50\n",
      "12392/12392 [==============================] - 104s 8ms/step - loss: 0.1485 - val_loss: 0.1766\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 256\n",
    "\n",
    "encoder_inputs = Input(shape = (None, len(nikkud_charset))) ## BONUS\n",
    "encoder_outputs, state_h, state_c = LSTM(latent_dim, return_state = True)(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape = (None, len(translit_charset))) ## BONUS\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences = True, return_state = True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state = encoder_states)\n",
    "decoder_dense = Dense(len(translit_charset), activation = 'softmax') ## BONUS\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy')\n",
    "model.fit([X, Y], Z, batch_size = 256, epochs = 50, validation_split = 0.2)\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_states_inputs = [\n",
    "    Input(shape = (latent_dim,)),\n",
    "    Input(shape = (latent_dim,))\n",
    "]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs,\n",
    "                                    initial_state = decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H80Wl0M4MgyU"
   },
   "source": [
    "**Question:**\n",
    "\n",
    "18. Check the input and output shapes of this model. What do these dimensions correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T11:11:39.314371Z",
     "start_time": "2019-08-17T11:11:38.920701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 46)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 31)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 310272      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  294912      input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 31)     7967        lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 613,151\n",
      "Trainable params: 613,151\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the input and output shapes are the shapes of the nikkud and transliteration vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r9HjABKj9cu7"
   },
   "source": [
    "Based on this model, we can decode transliteration from nikkud one character at a time, at each step taking the most likely next character predicted by the model. The function *nikkud2translit* takes in a nikkud string and returns the predicted transliteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T11:13:35.345436Z",
     "start_time": "2019-08-17T11:13:35.270852Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "P-lpIGu5LsvJ"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_text, input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1, len(translit_charset))) ## BONUS\n",
    "    target_seq[0, 0, 0] = 1.\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "        char_probabilities = {\n",
    "            c: p for c, p in zip(translit_charset, output_tokens[0, -1, :]) ## BONUS\n",
    "        }\n",
    "        sampled_char = max(translit_charset, key = lambda c: char_probabilities[c]) ## BONUS\n",
    "        sampled_token_index = translit_charset.index(sampled_char) ## BONUS\n",
    "        decoded_sentence += sampled_char\n",
    "        if (sampled_char == '$' or\n",
    "           len(decoded_sentence) > translit_maxlen): ## BONUS\n",
    "            stop_condition = True\n",
    "        target_seq = np.zeros((1, 1, len(translit_charset))) ## BONUS\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence\n",
    "\n",
    "def nikkud2translit(nikkud):\n",
    "  tensor = nikkud2onehot(nikkud)[None] ## BONUS\n",
    "  return decode_sequence(nikkud, tensor).replace('$', '').strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EE2GWL95BckG"
   },
   "source": [
    "**Questions:**\n",
    "  19. Make a new dataframe *df2* containing 100 random samples from *df*. Add a new column *predicted_translit* to the dataframe *df2* with the model's predicted transliteration of the given nikkud. How often does this equal the actual transliteration? What kinds of errors do you see in the output?\n",
    "  20. Change the value of *epochs =* above to train the model on more epochs. How does this affect the loss? How about the observed results?\n",
    "\n",
    "**Bonus:** Modify the problem so that we are instead predicting Hebrew text with nikkud from a transliteration. You will have to switch X and Y, and change code where the comment ## BONUS is written above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T11:15:04.914686Z",
     "start_time": "2019-08-17T11:15:04.848561Z"
    }
   },
   "outputs": [],
   "source": [
    "#19\n",
    "df2 = df.sample(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T11:16:56.640226Z",
     "start_time": "2019-08-17T11:16:45.106350Z"
    }
   },
   "outputs": [],
   "source": [
    "df2['predicted_translit'] = df2['nikkud'].apply(nikkud2translit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T11:19:59.953704Z",
     "start_time": "2019-08-17T11:19:59.933245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction equals the actual transliteration 37.0% of the time\n"
     ]
    }
   ],
   "source": [
    "acc = (df2['predicted_translit'] == df2['transliteration']).sum()/df2.shape[0]*100\n",
    "print(\"The prediction equals the actual transliteration {}% of the time\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T11:20:17.914361Z",
     "start_time": "2019-08-17T11:20:17.856700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nikkud</th>\n",
       "      <th>transliteration</th>\n",
       "      <th>word</th>\n",
       "      <th>predicted_translit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3567</th>\n",
       "      <td>דִּיסְטוֹפְּיָה</td>\n",
       "      <td>distopya</td>\n",
       "      <td>דיסטופיה</td>\n",
       "      <td>distopot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14734</th>\n",
       "      <td>שְׂפַת</td>\n",
       "      <td>sfat</td>\n",
       "      <td>שפת</td>\n",
       "      <td>sfat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10704</th>\n",
       "      <td>סְמַרְטוּט</td>\n",
       "      <td>smartut</td>\n",
       "      <td>סמרטוט</td>\n",
       "      <td>starmut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13342</th>\n",
       "      <td>קָרַחַת</td>\n",
       "      <td>karachat</td>\n",
       "      <td>קרחת</td>\n",
       "      <td>karkata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4097</th>\n",
       "      <td>הִיפֶּרְגְלִיקֶמְיָה</td>\n",
       "      <td>hiperglikemya</td>\n",
       "      <td>היפרגליקמיה</td>\n",
       "      <td>hiprekholit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     nikkud transliteration         word predicted_translit\n",
       "3567        דִּיסְטוֹפְּיָה        distopya     דיסטופיה           distopot\n",
       "14734                שְׂפַת            sfat          שפת               sfat\n",
       "10704            סְמַרְטוּט         smartut       סמרטוט            starmut\n",
       "13342               קָרַחַת        karachat         קרחת            karkata\n",
       "4097   הִיפֶּרְגְלִיקֶמְיָה   hiperglikemya  היפרגליקמיה        hiprekholit"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find mistakes such as some minor errors such as \"ch\" being predicted as \"k\" and sometimes predictions are very different while maintaining similar roots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T15:14:30.062066Z",
     "start_time": "2019-08-17T11:24:03.331157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12392 samples, validate on 3098 samples\n",
      "Epoch 1/70\n",
      "12392/12392 [==============================] - 127s 10ms/step - loss: 1.1970 - val_loss: 1.0903\n",
      "Epoch 2/70\n",
      "12392/12392 [==============================] - 115s 9ms/step - loss: 0.9529 - val_loss: 0.9518\n",
      "Epoch 3/70\n",
      "12392/12392 [==============================] - 173s 14ms/step - loss: 0.8909 - val_loss: 0.8397\n",
      "Epoch 4/70\n",
      "12392/12392 [==============================] - 94s 8ms/step - loss: 0.8294 - val_loss: 0.7525\n",
      "Epoch 5/70\n",
      "12392/12392 [==============================] - 94s 8ms/step - loss: 0.7713 - val_loss: 0.7026\n",
      "Epoch 6/70\n",
      "12392/12392 [==============================] - 100s 8ms/step - loss: 0.7270 - val_loss: 0.7240\n",
      "Epoch 7/70\n",
      "12392/12392 [==============================] - 94s 8ms/step - loss: 0.7013 - val_loss: 0.7189\n",
      "Epoch 8/70\n",
      "12392/12392 [==============================] - 92s 7ms/step - loss: 0.6838 - val_loss: 0.6619\n",
      "Epoch 9/70\n",
      "12392/12392 [==============================] - 101s 8ms/step - loss: 0.6701 - val_loss: 0.6463\n",
      "Epoch 10/70\n",
      "12392/12392 [==============================] - 114s 9ms/step - loss: 0.6626 - val_loss: 0.6464\n",
      "Epoch 11/70\n",
      "12392/12392 [==============================] - 102s 8ms/step - loss: 0.6545 - val_loss: 0.6437\n",
      "Epoch 12/70\n",
      "12392/12392 [==============================] - 98s 8ms/step - loss: 0.6480 - val_loss: 0.6466\n",
      "Epoch 13/70\n",
      "12392/12392 [==============================] - 97s 8ms/step - loss: 0.6407 - val_loss: 0.6738\n",
      "Epoch 14/70\n",
      "12392/12392 [==============================] - 93s 7ms/step - loss: 0.6364 - val_loss: 0.6689\n",
      "Epoch 15/70\n",
      "12392/12392 [==============================] - 98s 8ms/step - loss: 0.6321 - val_loss: 0.6285\n",
      "Epoch 16/70\n",
      "12392/12392 [==============================] - 986s 80ms/step - loss: 0.6279 - val_loss: 0.6213\n",
      "Epoch 17/70\n",
      "12392/12392 [==============================] - 93s 7ms/step - loss: 0.6197 - val_loss: 0.6263\n",
      "Epoch 18/70\n",
      "12392/12392 [==============================] - 94s 8ms/step - loss: 0.6070 - val_loss: 0.6258\n",
      "Epoch 19/70\n",
      "12392/12392 [==============================] - 93s 7ms/step - loss: 0.5969 - val_loss: 0.5992\n",
      "Epoch 20/70\n",
      "12392/12392 [==============================] - 95s 8ms/step - loss: 0.5863 - val_loss: 0.6268\n",
      "Epoch 21/70\n",
      "12392/12392 [==============================] - 116s 9ms/step - loss: 0.5735 - val_loss: 0.5711\n",
      "Epoch 22/70\n",
      "12392/12392 [==============================] - 95s 8ms/step - loss: 0.5552 - val_loss: 0.5538\n",
      "Epoch 23/70\n",
      "12392/12392 [==============================] - 632s 51ms/step - loss: 0.5384 - val_loss: 0.5410\n",
      "Epoch 24/70\n",
      "12392/12392 [==============================] - 96s 8ms/step - loss: 0.5243 - val_loss: 0.5382\n",
      "Epoch 25/70\n",
      "12392/12392 [==============================] - 94s 8ms/step - loss: 0.5113 - val_loss: 0.5316\n",
      "Epoch 26/70\n",
      "12392/12392 [==============================] - 93s 8ms/step - loss: 0.4933 - val_loss: 0.5328\n",
      "Epoch 27/70\n",
      "12392/12392 [==============================] - 91s 7ms/step - loss: 0.4831 - val_loss: 0.5187\n",
      "Epoch 28/70\n",
      "12392/12392 [==============================] - 94s 8ms/step - loss: 0.4636 - val_loss: 0.4499\n",
      "Epoch 29/70\n",
      "12392/12392 [==============================] - 94s 8ms/step - loss: 0.4368 - val_loss: 0.4307\n",
      "Epoch 30/70\n",
      "12392/12392 [==============================] - 181s 15ms/step - loss: 0.4358 - val_loss: 0.4290\n",
      "Epoch 31/70\n",
      "12392/12392 [==============================] - 93s 7ms/step - loss: 0.4214 - val_loss: 0.5479\n",
      "Epoch 32/70\n",
      "12392/12392 [==============================] - 91s 7ms/step - loss: 0.4047 - val_loss: 0.4210\n",
      "Epoch 33/70\n",
      "12392/12392 [==============================] - 93s 7ms/step - loss: 0.3986 - val_loss: 0.3802\n",
      "Epoch 34/70\n",
      "12392/12392 [==============================] - 99s 8ms/step - loss: 0.3736 - val_loss: 0.3978\n",
      "Epoch 35/70\n",
      "12392/12392 [==============================] - 111s 9ms/step - loss: 0.3782 - val_loss: 0.3945\n",
      "Epoch 36/70\n",
      "12392/12392 [==============================] - 92s 7ms/step - loss: 0.3508 - val_loss: 0.3451\n",
      "Epoch 37/70\n",
      "12392/12392 [==============================] - 95s 8ms/step - loss: 0.3372 - val_loss: 0.3557\n",
      "Epoch 38/70\n",
      "12392/12392 [==============================] - 92s 7ms/step - loss: 0.3248 - val_loss: 0.3296\n",
      "Epoch 39/70\n",
      "12392/12392 [==============================] - 94s 8ms/step - loss: 0.3179 - val_loss: 0.3309\n",
      "Epoch 40/70\n",
      "12392/12392 [==============================] - 94s 8ms/step - loss: 0.3005 - val_loss: 0.3073\n",
      "Epoch 41/70\n",
      "12392/12392 [==============================] - 91s 7ms/step - loss: 0.2905 - val_loss: 0.2933\n",
      "Epoch 42/70\n",
      "12392/12392 [==============================] - 1022s 82ms/step - loss: 0.2745 - val_loss: 0.5104\n",
      "Epoch 43/70\n",
      "12392/12392 [==============================] - 158s 13ms/step - loss: 0.2647 - val_loss: 0.2734\n",
      "Epoch 44/70\n",
      "12392/12392 [==============================] - 106s 9ms/step - loss: 0.2547 - val_loss: 0.2499\n",
      "Epoch 45/70\n",
      "12392/12392 [==============================] - 93s 8ms/step - loss: 0.2400 - val_loss: 0.2478\n",
      "Epoch 46/70\n",
      "12392/12392 [==============================] - 95s 8ms/step - loss: 0.2259 - val_loss: 0.2649\n",
      "Epoch 47/70\n",
      "12392/12392 [==============================] - 100s 8ms/step - loss: 0.2131 - val_loss: 0.2460\n",
      "Epoch 48/70\n",
      "12392/12392 [==============================] - 95s 8ms/step - loss: 0.2069 - val_loss: 0.2096\n",
      "Epoch 49/70\n",
      "12392/12392 [==============================] - 98s 8ms/step - loss: 0.1968 - val_loss: 0.2565\n",
      "Epoch 50/70\n",
      "12392/12392 [==============================] - 1864s 150ms/step - loss: 0.1701 - val_loss: 0.5115\n",
      "Epoch 51/70\n",
      "12392/12392 [==============================] - 263s 21ms/step - loss: 0.1798 - val_loss: 0.1988\n",
      "Epoch 52/70\n",
      "12392/12392 [==============================] - 160s 13ms/step - loss: 0.1682 - val_loss: 0.1820\n",
      "Epoch 53/70\n",
      "12392/12392 [==============================] - 120s 10ms/step - loss: 0.1641 - val_loss: 0.1881\n",
      "Epoch 54/70\n",
      "12392/12392 [==============================] - 126s 10ms/step - loss: 0.1601 - val_loss: 0.1601\n",
      "Epoch 55/70\n",
      "12392/12392 [==============================] - 93s 8ms/step - loss: 0.1330 - val_loss: 0.1620\n",
      "Epoch 56/70\n",
      "12392/12392 [==============================] - 92s 7ms/step - loss: 0.1508 - val_loss: 0.1818\n",
      "Epoch 57/70\n",
      "12392/12392 [==============================] - 91s 7ms/step - loss: 0.1410 - val_loss: 0.1733\n",
      "Epoch 58/70\n",
      "12392/12392 [==============================] - 98s 8ms/step - loss: 0.1325 - val_loss: 0.2547\n",
      "Epoch 59/70\n",
      "12392/12392 [==============================] - 95s 8ms/step - loss: 0.1135 - val_loss: 0.1601\n",
      "Epoch 60/70\n",
      "12392/12392 [==============================] - 98s 8ms/step - loss: 0.1266 - val_loss: 0.1561\n",
      "Epoch 61/70\n",
      "12392/12392 [==============================] - 512s 41ms/step - loss: 0.1277 - val_loss: 0.1365\n",
      "Epoch 62/70\n",
      "12392/12392 [==============================] - 106s 9ms/step - loss: 0.0985 - val_loss: 0.1426\n",
      "Epoch 63/70\n",
      "12392/12392 [==============================] - 94s 8ms/step - loss: 0.1140 - val_loss: 0.1419\n",
      "Epoch 64/70\n",
      "12392/12392 [==============================] - 93s 7ms/step - loss: 0.1114 - val_loss: 0.1403\n",
      "Epoch 65/70\n",
      "12392/12392 [==============================] - 92s 7ms/step - loss: 0.1058 - val_loss: 0.1257\n",
      "Epoch 66/70\n",
      "12392/12392 [==============================] - 95s 8ms/step - loss: 0.0807 - val_loss: 0.1313\n",
      "Epoch 67/70\n",
      "12392/12392 [==============================] - 94s 8ms/step - loss: 0.1061 - val_loss: 0.1275\n",
      "Epoch 68/70\n",
      "12392/12392 [==============================] - 95s 8ms/step - loss: 0.0975 - val_loss: 0.1184\n",
      "Epoch 69/70\n",
      "12392/12392 [==============================] - 2074s 167ms/step - loss: 0.0740 - val_loss: 0.1266\n",
      "Epoch 70/70\n",
      "12392/12392 [==============================] - 111s 9ms/step - loss: 0.0918 - val_loss: 0.1220\n"
     ]
    }
   ],
   "source": [
    "#20\n",
    "latent_dim = 256\n",
    "\n",
    "encoder_inputs = Input(shape = (None, len(nikkud_charset))) ## BONUS\n",
    "encoder_outputs, state_h, state_c = LSTM(latent_dim, return_state = True)(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape = (None, len(translit_charset))) ## BONUS\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences = True, return_state = True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state = encoder_states)\n",
    "decoder_dense = Dense(len(translit_charset), activation = 'softmax') ## BONUS\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy')\n",
    "model.fit([X, Y], Z, batch_size = 256, epochs = 70, validation_split = 0.2)\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_states_inputs = [\n",
    "    Input(shape = (latent_dim,)),\n",
    "    Input(shape = (latent_dim,))\n",
    "]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs,\n",
    "                                    initial_state = decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss drops since after 50 epochs we were still underfitting (there was no sign of overfitting since the train and validation loss were still dropping). Therefore the observed results are more accurate as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T15:20:07.786206Z",
     "start_time": "2019-08-17T15:19:51.630558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction equals the actual transliteration 54.0% of the time\n"
     ]
    }
   ],
   "source": [
    "df3 = df.sample(n=100)\n",
    "df3['predicted_translit'] = df3['nikkud'].apply(nikkud2translit)\n",
    "acc = (df3['predicted_translit'] == df3['transliteration']).sum()/df3.shape[0]*100\n",
    "print(\"The prediction equals the actual transliteration {}% of the time\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NMT exercise 2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
