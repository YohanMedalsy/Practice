{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HhhgpxxSl8Ka"
   },
   "source": [
    "# NER Workshop Exercise 2: Training a Custom NER Algorithm\n",
    "\n",
    "In this exercise, we will train our own RNN-based Named Entity Recognition algorithm, using the CoNLL-2003 tagged dataset.\n",
    "\n",
    "## Part 1: Loading CoNLL-2003 data\n",
    "\n",
    "The [CoNLL-2003](https://www.clips.uantwerpen.be/conll2003/ner/) shared task was a joint effort by academics to provide approaches to named entity recognition, using a tagged dataset of named entities in English and German. We will be using the tagged English data from CoNLL-2003, found in the accompanying file *conll2003.zip*.\n",
    "\n",
    "After uploading this file to the current directory, access the data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T19:02:05.887439Z",
     "start_time": "2019-08-16T19:02:05.843565Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7978,
     "status": "ok",
     "timestamp": 1565061676526,
     "user": {
      "displayName": "Morris Alper",
      "photoUrl": "https://lh3.googleusercontent.com/--_gcOdCIoAM/AAAAAAAAAAI/AAAAAAAAFCI/ar-HeAB3FNk/s64/photo.jpg",
      "userId": "15842932163458061285"
     },
     "user_tz": -180
    },
    "id": "FuXHLev_7DzL",
    "outputId": "32d06446-b0d8-4cbe-ced3-082d60ebea8f"
   },
   "outputs": [],
   "source": [
    "# ! unzip conll2003.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T19:02:35.743127Z",
     "start_time": "2019-08-16T19:02:08.789061Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "PiEEWY2Gd6B4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def read_conll(filename):\n",
    "  df = pd.read_csv(filename,\n",
    "                     sep = ' ', header = None, keep_default_na = False,\n",
    "                     names = ['TOKEN', 'POS', 'CHUNK', 'NE'],\n",
    "                     quoting = 3, skip_blank_lines = False)\n",
    "  df['SENTENCE'] = (df.TOKEN == '').cumsum()\n",
    "  return df[df.TOKEN != '']\n",
    "train_df = read_conll('conll2003/train.txt')\n",
    "valid_df = read_conll('conll2003/valid.txt')\n",
    "test_df = read_conll('conll2003/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TUC6Fe3IfeyU"
   },
   "source": [
    "Note that the CoNLL-2003 data contains part-of-speech (POS) and chunk tags, but we will only be using the token text and named entity (NE) tags that are provided.\n",
    "\n",
    "**Questions:**\n",
    "  1. What percentages of the CoNLL-2003 data are training, validation, and testing data? (calculate directly)\n",
    "  2. What do the tags in column 'NE' mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T19:02:36.209068Z",
     "start_time": "2019-08-16T19:02:36.135823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the train set is 67.55600027740076% of the data\n",
      "the test set is 15.410932892134038% of the data\n",
      "the validation set is 17.033066830465206% of the data\n"
     ]
    }
   ],
   "source": [
    "train_size = train_df.shape[0]\n",
    "test_size = test_df.shape[0]\n",
    "valid_size = valid_df.shape[0]\n",
    "colon_size = train_size + test_size + valid_size\n",
    "train_per = train_size/colon_size*100\n",
    "test_per = test_size/colon_size*100\n",
    "valid_per = valid_size/colon_size*100\n",
    "print(\"the train set is {}% of the data\".format(train_per))\n",
    "print(\"the test set is {}% of the data\".format(test_per))\n",
    "print(\"the validation set is {}% of the data\".format(valid_per))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:58:02.372428Z",
     "start_time": "2019-08-06T11:58:02.339235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O         170524\n",
       "B-LOC       7140\n",
       "B-PER       6600\n",
       "B-ORG       6321\n",
       "I-PER       4528\n",
       "I-ORG       3704\n",
       "B-MISC      3438\n",
       "I-LOC       1157\n",
       "I-MISC      1155\n",
       "Name: NE, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['NE'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above list, the B stands for Beginning of the sequence, I for Inside the sequence and O stands for Outside. \n",
    "Loc is Location, PER is Person, ORG is Organization, Misc is miscellaneous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dt8U2DEdgpV4"
   },
   "source": [
    "## Part 2: Feature calculation\n",
    "\n",
    "In order to learn named entity recognition using RNNs, we must transform our input and output into numeric vectors by calculating relevant features. For our basic NER algorithm, we will simply use word indices as input and one-hot embeddings of NER tags as output.\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "3. Save a list of the 5000 most common word tokens (values from column 'TOKEN') in our training data as a list 'vocab', and save a list of all unique entity tags (values from column 'NE') as a list 'ne_tags'. \n",
    "4. Create a function token2index(token) that takes in the value of a word token and returns a unique integer. It should return 1 for any token which is not found in 'vocab' (i.e. which is out-of-vocabulary) and a number >= 2 for every token found in 'vocab'.\n",
    "5. Create a function ne_tag2index(ne_tag) which returns a unique integer >= 1 for every entity tag.\n",
    "6. Add new columns 'token_index' and 'ne_index' to the CoNLL data DataFrames containing the values of token2index() and ne_tag2index() for each token and entity tag.\n",
    "7. Generate training data feature matrix X_train of size (14987, 50) as follows:\n",
    "  * Use train_df.groupby('SENTENCE').token_index.apply(list) to get a list of lists of token indices, one list for each sentence.\n",
    "  * Use pad_sequences() from keras.preprocessing.sequence to pad every list of token indices with the value '0' at the beginning so they are all of length 50.\n",
    "8. Generate output data feature matrix Y_train of size (14987, 50, 10) by applying the same method to the entity token indices (column 'ne_index'), and then one-hot encoding using to_categorical() from keras.utils.\n",
    "9. Apply 7-8 on the validation and testing data as well to generate matrices X_valid, Y_valid, X_test, Y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:58:03.813625Z",
     "start_time": "2019-08-06T11:58:03.736539Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "vocab = Counter(train_df['TOKEN']).most_common(5000)\n",
    "vocab = [word[0] for word in vocab]\n",
    "ne_tags = list(train_df['NE'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:58:05.009172Z",
     "start_time": "2019-08-06T11:58:05.003818Z"
    }
   },
   "outputs": [],
   "source": [
    "def token2index(token):\n",
    "    if token in vocab:\n",
    "        return vocab.index(token) + 2\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:58:05.821197Z",
     "start_time": "2019-08-06T11:58:05.816081Z"
    }
   },
   "outputs": [],
   "source": [
    "def ne_tag2index(ne_tag):\n",
    "    return ne_tags.index(ne_tag) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new columns 'token_index' and 'ne_index' to the CoNLL data DataFrames containing the values of token2index() and ne_tag2index() for each token and entity tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:58:21.306255Z",
     "start_time": "2019-08-06T11:58:06.587223Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df[\"token_index\"] = train_df['TOKEN'].apply(token2index)\n",
    "train_df[\"ne_index\"] = train_df['NE'].apply(ne_tag2index)\n",
    "\n",
    "test_df[\"token_index\"] = test_df['TOKEN'].apply(token2index)\n",
    "test_df[\"ne_index\"] = test_df['NE'].apply(ne_tag2index)\n",
    "\n",
    "valid_df[\"token_index\"] = valid_df['TOKEN'].apply(token2index)\n",
    "valid_df[\"ne_index\"] = valid_df['NE'].apply(ne_tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:58:24.027862Z",
     "start_time": "2019-08-06T11:58:22.226955Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "new_train_df = train_df.groupby('SENTENCE').token_index.apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:58:25.109499Z",
     "start_time": "2019-08-06T11:58:25.019221Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = pad_sequences(new_train_df.to_list(), value = 0, maxlen = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:58:26.800618Z",
     "start_time": "2019-08-06T11:58:25.976518Z"
    }
   },
   "outputs": [],
   "source": [
    "new_test_df = test_df.groupby('SENTENCE').token_index.apply(list)\n",
    "new_valid_df = valid_df.groupby('SENTENCE').token_index.apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:58:27.684708Z",
     "start_time": "2019-08-06T11:58:27.638439Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = pad_sequences(new_test_df.to_list(), value = 0, maxlen = 50)\n",
    "X_valid = pad_sequences(new_valid_df.to_list(), value = 0, maxlen = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:58:31.033323Z",
     "start_time": "2019-08-06T11:58:28.656279Z"
    }
   },
   "outputs": [],
   "source": [
    "new_train_df2 = train_df.groupby('SENTENCE').ne_index.apply(list)\n",
    "new_test_df2 = test_df.groupby('SENTENCE').ne_index.apply(list)\n",
    "new_valid_df2 = valid_df.groupby('SENTENCE').ne_index.apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:58:31.983312Z",
     "start_time": "2019-08-06T11:58:31.856497Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = pad_sequences(new_train_df2.to_list(), value = 0, maxlen = 50)\n",
    "y_test = pad_sequences(new_test_df2.to_list(), value = 0, maxlen = 50)\n",
    "y_valid = pad_sequences(new_valid_df2.to_list(), value = 0, maxlen = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:58:32.844239Z",
     "start_time": "2019-08-06T11:58:32.767439Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_valid = to_categorical(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:58:33.660554Z",
     "start_time": "2019-08-06T11:58:33.653766Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14987, 50, 10)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GhYllqeCrrCE"
   },
   "source": [
    "## Part 3: Building and training the model\n",
    "\n",
    "Now we are ready to build our network that will predict NER tags from the inputted words. The architecture will be roughly similar to our previous exercise on RNNs.\n",
    "\n",
    "The following imports will help you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:58:34.697539Z",
     "start_time": "2019-08-06T11:58:34.692081Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "DhR9m-Nit5_k"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, TimeDistributed, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:58:36.050582Z",
     "start_time": "2019-08-06T11:58:35.567001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 200)           1000400   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50, 128)           168448    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 50, 10)            1290      \n",
      "=================================================================\n",
      "Total params: 1,170,138\n",
      "Trainable params: 1,170,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(mask_zero = True, input_dim = len(vocab) + 2, output_dim=200, input_length=50))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(len(ne_tags) + 1, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q4VXf9qRt9oy"
   },
   "source": [
    "**Questions:**\n",
    "\n",
    "10. Build a sequential model 'model', and add the following layers with *model.add()*:\n",
    "  * Embedding -- use embedding dimension 200, and make sure to set *input_length = 50* and *mask_zero = True* (to ignore the padding indices).\n",
    "  * LSTM -- use hidden state dimension 128, and return the hidden state at each time step (*return_sequences = True*)\n",
    "  * Fully-connected layer (*Dense()*) with softmax activation. Make sure that this is wrapped in *TimeDistributed()* so that it is applied to the output of our LSTM at each time step. Hint: The output dimension of *Dense* is the number of possible output labels, including the padding label '0'.\n",
    "\n",
    "  Compile the model (*model.compile()*) with loss function 'categorical_crossentropy' and optimizer 'adam', and print a summary of the model (*model.summary()*). What is the expected shape of input for the model? (Hint: see *model.input_shape*, where *None* means that any number is allowed.)\n",
    "11. Run the code below in (A) to train the model, changing the number of epochs so the model learns until it starts overfitting. How many epochs did you use for training?\n",
    "12. Create a model *model2* that is the same as *model* but with the LSTM layer wrapped by *Bidirectional()*, so the model becomes a BiLSTM model. How does this change the final validation loss? Does the model improve?\n",
    "13. Compare the performance of the two models on the test set data X_test and Y_test (Hint: use model.evaluate())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T12:03:43.605556Z",
     "start_time": "2019-08-06T11:58:37.241181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14987 samples, validate on 3466 samples\n",
      "Epoch 1/10\n",
      "14987/14987 [==============================] - 32s 2ms/step - loss: 0.8932 - val_loss: 0.5719\n",
      "Epoch 2/10\n",
      "14987/14987 [==============================] - 31s 2ms/step - loss: 0.4177 - val_loss: 0.4012\n",
      "Epoch 3/10\n",
      "14987/14987 [==============================] - 30s 2ms/step - loss: 0.2872 - val_loss: 0.2758\n",
      "Epoch 4/10\n",
      "14987/14987 [==============================] - 29s 2ms/step - loss: 0.2009 - val_loss: 0.2315\n",
      "Epoch 5/10\n",
      "14987/14987 [==============================] - 31s 2ms/step - loss: 0.1594 - val_loss: 0.2161\n",
      "Epoch 6/10\n",
      "14987/14987 [==============================] - 30s 2ms/step - loss: 0.1387 - val_loss: 0.2065\n",
      "Epoch 7/10\n",
      "14987/14987 [==============================] - 29s 2ms/step - loss: 0.1260 - val_loss: 0.2110\n",
      "Epoch 8/10\n",
      "14987/14987 [==============================] - 29s 2ms/step - loss: 0.1162 - val_loss: 0.2142\n",
      "Epoch 9/10\n",
      "14987/14987 [==============================] - 30s 2ms/step - loss: 0.1076 - val_loss: 0.2066\n",
      "Epoch 10/10\n",
      "14987/14987 [==============================] - 35s 2ms/step - loss: 0.1012 - val_loss: 0.2192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb3fcf5a90>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#11 (A) \n",
    "model.fit(X_train, y_train, epochs = 10, batch_size = 128, validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to my view, after 6 epochs the validation loss bottoms. After 6 it beings to oscillate until it starts strictly rising. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T12:10:45.900997Z",
     "start_time": "2019-08-06T12:10:43.203308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 50, 200)           1000400   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 256)           336896    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 50, 10)            2570      \n",
      "=================================================================\n",
      "Total params: 1,339,866\n",
      "Trainable params: 1,339,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(mask_zero = True, input_dim = len(vocab) + 2, output_dim=200, input_length=50))\n",
    "model2.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model2.add(TimeDistributed(Dense(len(ne_tags) + 1, activation='softmax')))\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T12:22:16.005133Z",
     "start_time": "2019-08-06T12:10:55.634198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14987 samples, validate on 3466 samples\n",
      "Epoch 1/10\n",
      "14987/14987 [==============================] - 65s 4ms/step - loss: 0.8264 - val_loss: 0.5262\n",
      "Epoch 2/10\n",
      "14987/14987 [==============================] - 57s 4ms/step - loss: 0.3631 - val_loss: 0.3176\n",
      "Epoch 3/10\n",
      "14987/14987 [==============================] - 58s 4ms/step - loss: 0.2028 - val_loss: 0.2223\n",
      "Epoch 4/10\n",
      "14987/14987 [==============================] - 59s 4ms/step - loss: 0.1400 - val_loss: 0.1874\n",
      "Epoch 5/10\n",
      "14987/14987 [==============================] - 63s 4ms/step - loss: 0.1104 - val_loss: 0.1984\n",
      "Epoch 6/10\n",
      "14987/14987 [==============================] - 73s 5ms/step - loss: 0.0916 - val_loss: 0.1725\n",
      "Epoch 7/10\n",
      "14987/14987 [==============================] - 80s 5ms/step - loss: 0.0774 - val_loss: 0.1693\n",
      "Epoch 8/10\n",
      "14987/14987 [==============================] - 64s 4ms/step - loss: 0.0652 - val_loss: 0.1735\n",
      "Epoch 9/10\n",
      "14987/14987 [==============================] - 69s 5ms/step - loss: 0.0561 - val_loss: 0.1827\n",
      "Epoch 10/10\n",
      "14987/14987 [==============================] - 91s 6ms/step - loss: 0.0481 - val_loss: 0.1746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb41b45e48>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, y_train, epochs = 10, batch_size = 128, validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the model improves as we can see that the validation and training loss decrease in model2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T12:24:51.573265Z",
     "start_time": "2019-08-06T12:24:45.118377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684/3684 [==============================] - 6s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T12:25:25.210479Z",
     "start_time": "2019-08-06T12:25:15.858802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684/3684 [==============================] - 9s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "score2 = model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T12:27:54.385927Z",
     "start_time": "2019-08-06T12:27:54.366005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Test loss: 0.3178489805602678\n",
      "Model 2 Test loss: 0.2805885224725473\n"
     ]
    }
   ],
   "source": [
    "print('Model 1 Test loss:', score)\n",
    "print('Model 2 Test loss:', score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T12:27:44.617971Z",
     "start_time": "2019-08-06T12:27:44.600762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3178489805602678"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F0q7o8wJzUOb"
   },
   "source": [
    "## Bonus 1: Running on custom input\n",
    "\n",
    "**Bonus question 1:**\n",
    "\n",
    "What does your model predict as NER tags for the following test sentences?\n",
    "\n",
    "Hint: Try using the following pipeline on each sentence:\n",
    "\n",
    "* Tokenize with nltk.word_tokenize()\n",
    "* Convert to array of indices with word2index() defined above\n",
    "* Pad to length 50 with pad_sequences() from Keras\n",
    "* Predict probabilities of NER tags with model2.predict()\n",
    "* Find maximum likelihood tags using np.argmax() (with axis = 1), and ignore padding values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:09:30.659833Z",
     "start_time": "2019-08-06T11:08:52.958Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5dXPLnjyzV7z"
   },
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "  \"This is a test.\",\n",
    "  \"I live in the United States.\",\n",
    "  \"Israel is a country in the Middle East.\",\n",
    "  \"UK joins US in Gulf mission after Iran taunts American allies\",\n",
    "  \"The project was funded by EuroNanoMed-II, the Health Ministry, the Portuguese Foundation for Science and Technology, the Israel Science Foundation, the European Research Council’s Consolidator and Advanced Awards, the Saban Family Foundation – Melanoma Research Alliance’s Team Science Award and the Israel Cancer Research Fund.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LuKz5eC8zXvj"
   },
   "source": [
    "## Bonus 2: Adding features\n",
    "\n",
    "**Bonus question 2:**\n",
    "\n",
    "In (B) below, add code to add a new column 'SHAPE' to the dataset. This column should represent the shape of the word token by:\n",
    "* Replacing all capital letters with 'X'\n",
    "* Replacing all lowercase letters with 'x'\n",
    "* Replacing all digits with 'd'\n",
    "\n",
    "For example, we should have the following:\n",
    "\n",
    "* 'house' => 'xxxxx'\n",
    "* 'Apple' => 'Xxxxx'\n",
    "* 'R2D2' => 'XdXd'\n",
    "* 'U.K.' => 'X.X.'\n",
    "\n",
    "Hint: for a Pandas series. you can use series.str.replace() to easily replace text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:09:30.661886Z",
     "start_time": "2019-08-06T11:08:52.997Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "uLvg69DT0T6Q"
   },
   "outputs": [],
   "source": [
    "def series2shape(series):\n",
    "  ## (B) -- add bonus question code here\n",
    "  \n",
    "train_df['SHAPE'] = series2shape(train_df.TOKEN)\n",
    "valid_df['SHAPE'] = series2shape(valid_df.TOKEN)\n",
    "test_df['SHAPE'] = series2shape(test_df.TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-OTRRNls0m0k"
   },
   "source": [
    "Once you complete this, run the following code to see how adding this as a feature improves the performance of the model. For simplicity we only use the top 100 word shapes. How does the final loss change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:09:30.664253Z",
     "start_time": "2019-08-06T11:08:53.041Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "3uHWVLy7-SCb"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "shape_vocab = [w for w, f in FreqDist(train_df.SHAPE).most_common(n = 100)]\n",
    "shape_set = set(shape_vocab)\n",
    "def shape2index(shape):\n",
    "  if shape in shape_set:\n",
    "    return shape_vocab.index(shape) + 2\n",
    "  else: # out-of-vocabulary shape\n",
    "    return 1\n",
    "\n",
    "n_words = 50\n",
    "def df2features2(df):\n",
    "  tqdm.pandas('Shape indices')\n",
    "  df['shape_index'] = df.SHAPE.progress_apply(shape2index)\n",
    "  token_index_lists = df.groupby('SENTENCE').token_index.apply(list)\n",
    "  ne_index_lists = df.groupby('SENTENCE').ne_index.apply(list)\n",
    "  shape_index_lists = df.groupby('SENTENCE').ne_index.apply(list)\n",
    "  X = np.stack([\n",
    "      pad_sequences(token_index_lists, maxlen = n_words, value = 0),\n",
    "      pad_sequences(shape_index_lists, maxlen = n_words, value = 0)\n",
    "  ])\n",
    "  Y = to_categorical(pad_sequences(ne_index_lists, maxlen = n_words, value = 0))\n",
    "  return X, Y\n",
    "\n",
    "X2_train, Y2_train = df2features2(train_df)\n",
    "X2_valid, Y2_valid = df2features2(valid_df)\n",
    "X2_test, Y2_test = df2features2(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:09:30.666010Z",
     "start_time": "2019-08-06T11:08:53.082Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "rAGrKU9K2lAB"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate\n",
    "\n",
    "input1 = Input(shape = (50,))\n",
    "input2 = Input(shape = (50,))\n",
    "embedded1 = Embedding(\n",
    "    len(vocab) + 2, 200,\n",
    "    input_length = 50, mask_zero = True)(input1)\n",
    "embedded2 = Embedding(\n",
    "    len(shape_vocab) + 2, 8,\n",
    "    input_length = 50, mask_zero = True)(input2)\n",
    "x = concatenate([embedded1, embedded2])\n",
    "x = Bidirectional(LSTM(128, return_sequences = True))(x)\n",
    "output = TimeDistributed(Dense(len(ne_tags) + 1, activation = 'softmax'))(x)\n",
    "model3 = Model(inputs = [input1, input2], outputs = [output])\n",
    "model3.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:09:30.667899Z",
     "start_time": "2019-08-06T11:08:53.116Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "GT-44jlr-UYD"
   },
   "outputs": [],
   "source": [
    "model3.fit(\n",
    "    [X2_train[0], X2_train[1]],\n",
    "    Y2_train, epochs = 5, batch_size = 128,\n",
    "    validation_data = ([X2_valid[0], X2_valid[1]], Y2_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:09:30.669579Z",
     "start_time": "2019-08-06T11:08:53.139Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "WdkqGaVz-WS7"
   },
   "outputs": [],
   "source": [
    "print(\"Model3 loss on test data:\")\n",
    "model3.evaluate([X2_test[0], X2_test[1]], Y2_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NER Exercise 2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
