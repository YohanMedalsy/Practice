{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DSWzy0w5ESWk"
   },
   "source": [
    "# Exercise 2: Building a \"little stemmer\"\n",
    "\n",
    "For this exercise, we will take a sample of Antoine de Saint-ExupÃ©ry's novella *The Little Prince* and use it to demonstrate tokenization and stemming.\n",
    "\n",
    "Here is your sample text, which appears at the beginning of the book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T21:39:36.878021Z",
     "start_time": "2019-08-10T21:39:36.873550Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "38EI6SxlDzZR"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Once when I was six years old I saw a magnificent picture in a book, called True Stories from Nature, about the primeval forest. It was a picture of a boa constrictor in the act of swallowing an animal. Here is a copy of the drawing.\n",
    "Boa\n",
    "In the book it said: \"Boa constrictors swallow their prey whole, without chewing it. After that they are not able to move, and they sleep through the six months that they need for digestion.\"\n",
    "I pondered deeply, then, over the adventures of the jungle. And after some work with a colored pencil I succeeded in making my first drawing. My Drawing Number One. It looked something like this:\n",
    "Hat\n",
    "I showed my masterpiece to the grown-ups, and asked them whether the drawing frightened them.\n",
    "But they answered: \"Frighten? Why should any one be frightened by a hat?\"\n",
    "My drawing was not a picture of a hat. It was a picture of a boa constrictor digesting an elephant. But since the grown-ups were not able to understand it, I made another drawing: I drew the inside of a boa constrictor, so that the grown-ups could see it clearly. They always need to have things explained. My Drawing Number Two looked like this:\n",
    "Elephant inside the boa\n",
    "The grown-ups' response, this time, was to advise me to lay aside my drawings of boa constrictors, whether from the inside or the outside, and devote myself instead to geography, history, arithmetic, and grammar. That is why, at the age of six, I gave up what might have been a magnificent career as a painter. I had been disheartened by the failure of my Drawing Number One and my Drawing Number Two. Grown-ups never understand anything by themselves, and it is tiresome for children to be always and forever explaining things to them.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XaxfIOKzFUwy"
   },
   "source": [
    "First let's use NLTK's build-in functions to tokenize and stem this text. First convert the given text into an array of lowercase tokens using the NLTK functions word_tokenize and PorterStemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T21:39:41.477075Z",
     "start_time": "2019-08-10T21:39:36.901248Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "duCI57c5B0m1"
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T21:41:58.370207Z",
     "start_time": "2019-08-10T21:41:58.357481Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "text_tokens = word_tokenize(text, language='English')\n",
    "lower_tokens = [word.lower() for word in text_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VZ4Hh5JpIM7w"
   },
   "source": [
    "**Questions:**\n",
    "  1. How many unique tokens are there in the text?\n",
    "  1. How many unique stemmed tokens are in the text? Lowercase stemmed tokens?\n",
    "  1. What are some examples of words that have surprising stemmed forms? Can you explain why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T21:39:43.343538Z",
     "start_time": "2019-08-10T21:39:41.539832Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T21:42:08.776430Z",
     "start_time": "2019-08-10T21:42:08.771116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 155 unique tokens in the text\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} unique tokens in the text\".format(len(set(lower_tokens))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T21:39:43.382261Z",
     "start_time": "2019-08-10T21:39:43.359260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 152 unique stemmed tokens\n",
      "There are 149 unique lowercase stemmed tokens\n"
     ]
    }
   ],
   "source": [
    "stems_lower = set([stemmer.stem(word) for word in lower_tokens])\n",
    "stems = set([stemmer.stem(word) for word in text_tokens])\n",
    "\n",
    "print(\"There are {} unique stemmed tokens\".format(len(stems)))\n",
    "print(\"There are {} unique lowercase stemmed tokens\".format(len(stems_lower)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of words that have surprising stemmed tokens are once, is, was, animal, able, something, this.\n",
    "It is surprising because these words have common english suffixes that would normally appear at the end of verbs and such but they are part of the real root of the word. These are simply cases where stemming fails in capturing the root meaning of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jec0fStwKy5q"
   },
   "source": [
    "Now let's try writing our own stemmer. Write a function which takes in a token and returns its stem, by removing common English suffixes (e.g. remove the suffix -ed as in *listened* -> *listen*). Try to handle as many suffixes as you can think of. Then use this custom stemmer to convert the given text to an array of lowercase tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T21:39:43.432060Z",
     "start_time": "2019-08-10T21:39:43.400679Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "IUN-s5LuEJdo"
   },
   "outputs": [],
   "source": [
    "def suffix_word(my_word, suf_list):\n",
    "    for my_suf in suf_list:\n",
    "        suf_len = len(my_suf)\n",
    "        if my_word[-suf_len:] == my_suf:\n",
    "            return my_word[0:-len(my_suf)].lower()\n",
    "    return my_word.lower()\n",
    "        \n",
    "def stem_text(my_text):\n",
    "    my_words = re.findall(r\"[\\w']+|[.,!?;:-]\", my_text)\n",
    "    my_suffixes = ['ed', 'es', 'ing', 'e', 's', 'ent', 'al', 'ion', 'y']\n",
    "    \n",
    "    return [suffix_word(word, my_suffixes) for word in my_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T21:39:43.532179Z",
     "start_time": "2019-08-10T21:39:43.466191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['onc',\n",
       " 'when',\n",
       " 'i',\n",
       " 'wa',\n",
       " 'six',\n",
       " 'year',\n",
       " 'old',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'a',\n",
       " 'magnific',\n",
       " 'pictur',\n",
       " 'in',\n",
       " 'a',\n",
       " 'book',\n",
       " ',',\n",
       " 'call',\n",
       " 'tru',\n",
       " 'stori',\n",
       " 'from',\n",
       " 'natur',\n",
       " ',',\n",
       " 'about',\n",
       " 'th',\n",
       " 'primev',\n",
       " 'forest',\n",
       " '.',\n",
       " 'it',\n",
       " 'wa',\n",
       " 'a',\n",
       " 'pictur',\n",
       " 'of',\n",
       " 'a',\n",
       " 'boa',\n",
       " 'constrictor',\n",
       " 'in',\n",
       " 'th',\n",
       " 'act',\n",
       " 'of',\n",
       " 'swallow',\n",
       " 'an',\n",
       " 'anim',\n",
       " '.',\n",
       " 'her',\n",
       " 'i',\n",
       " 'a',\n",
       " 'cop',\n",
       " 'of',\n",
       " 'th',\n",
       " 'draw',\n",
       " '.',\n",
       " 'boa',\n",
       " 'in',\n",
       " 'th',\n",
       " 'book',\n",
       " 'it',\n",
       " 'said',\n",
       " ':',\n",
       " 'boa',\n",
       " 'constrictor',\n",
       " 'swallow',\n",
       " 'their',\n",
       " 'pre',\n",
       " 'whol',\n",
       " ',',\n",
       " 'without',\n",
       " 'chew',\n",
       " 'it',\n",
       " '.',\n",
       " 'after',\n",
       " 'that',\n",
       " 'the',\n",
       " 'ar',\n",
       " 'not',\n",
       " 'abl',\n",
       " 'to',\n",
       " 'mov',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'sleep',\n",
       " 'through',\n",
       " 'th',\n",
       " 'six',\n",
       " 'month',\n",
       " 'that',\n",
       " 'the',\n",
       " 'ne',\n",
       " 'for',\n",
       " 'digest',\n",
       " '.',\n",
       " 'i',\n",
       " 'ponder',\n",
       " 'deepl',\n",
       " ',',\n",
       " 'then',\n",
       " ',',\n",
       " 'over',\n",
       " 'th',\n",
       " 'adventur',\n",
       " 'of',\n",
       " 'th',\n",
       " 'jungl',\n",
       " '.',\n",
       " 'and',\n",
       " 'after',\n",
       " 'som',\n",
       " 'work',\n",
       " 'with',\n",
       " 'a',\n",
       " 'color',\n",
       " 'pencil',\n",
       " 'i',\n",
       " 'succeed',\n",
       " 'in',\n",
       " 'mak',\n",
       " 'm',\n",
       " 'first',\n",
       " 'draw',\n",
       " '.',\n",
       " 'm',\n",
       " 'draw',\n",
       " 'number',\n",
       " 'on',\n",
       " '.',\n",
       " 'it',\n",
       " 'look',\n",
       " 'someth',\n",
       " 'lik',\n",
       " 'thi',\n",
       " ':',\n",
       " 'hat',\n",
       " 'i',\n",
       " 'show',\n",
       " 'm',\n",
       " 'masterpiec',\n",
       " 'to',\n",
       " 'th',\n",
       " 'grown',\n",
       " '-',\n",
       " 'up',\n",
       " ',',\n",
       " 'and',\n",
       " 'ask',\n",
       " 'them',\n",
       " 'whether',\n",
       " 'th',\n",
       " 'draw',\n",
       " 'frighten',\n",
       " 'them',\n",
       " '.',\n",
       " 'but',\n",
       " 'the',\n",
       " 'answer',\n",
       " ':',\n",
       " 'frighten',\n",
       " '?',\n",
       " 'wh',\n",
       " 'should',\n",
       " 'an',\n",
       " 'on',\n",
       " 'b',\n",
       " 'frighten',\n",
       " 'b',\n",
       " 'a',\n",
       " 'hat',\n",
       " '?',\n",
       " 'm',\n",
       " 'draw',\n",
       " 'wa',\n",
       " 'not',\n",
       " 'a',\n",
       " 'pictur',\n",
       " 'of',\n",
       " 'a',\n",
       " 'hat',\n",
       " '.',\n",
       " 'it',\n",
       " 'wa',\n",
       " 'a',\n",
       " 'pictur',\n",
       " 'of',\n",
       " 'a',\n",
       " 'boa',\n",
       " 'constrictor',\n",
       " 'digest',\n",
       " 'an',\n",
       " 'elephant',\n",
       " '.',\n",
       " 'but',\n",
       " 'sinc',\n",
       " 'th',\n",
       " 'grown',\n",
       " '-',\n",
       " 'up',\n",
       " 'wer',\n",
       " 'not',\n",
       " 'abl',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'it',\n",
       " ',',\n",
       " 'i',\n",
       " 'mad',\n",
       " 'another',\n",
       " 'draw',\n",
       " ':',\n",
       " 'i',\n",
       " 'drew',\n",
       " 'th',\n",
       " 'insid',\n",
       " 'of',\n",
       " 'a',\n",
       " 'boa',\n",
       " 'constrictor',\n",
       " ',',\n",
       " 'so',\n",
       " 'that',\n",
       " 'th',\n",
       " 'grown',\n",
       " '-',\n",
       " 'up',\n",
       " 'could',\n",
       " 'se',\n",
       " 'it',\n",
       " 'clearl',\n",
       " '.',\n",
       " 'the',\n",
       " 'alway',\n",
       " 'ne',\n",
       " 'to',\n",
       " 'hav',\n",
       " 'thing',\n",
       " 'explain',\n",
       " '.',\n",
       " 'm',\n",
       " 'draw',\n",
       " 'number',\n",
       " 'two',\n",
       " 'look',\n",
       " 'lik',\n",
       " 'thi',\n",
       " ':',\n",
       " 'elephant',\n",
       " 'insid',\n",
       " 'th',\n",
       " 'boa',\n",
       " 'th',\n",
       " 'grown',\n",
       " '-',\n",
       " \"ups'\",\n",
       " 'respons',\n",
       " ',',\n",
       " 'thi',\n",
       " 'tim',\n",
       " ',',\n",
       " 'wa',\n",
       " 'to',\n",
       " 'advis',\n",
       " 'm',\n",
       " 'to',\n",
       " 'la',\n",
       " 'asid',\n",
       " 'm',\n",
       " 'drawing',\n",
       " 'of',\n",
       " 'boa',\n",
       " 'constrictor',\n",
       " ',',\n",
       " 'whether',\n",
       " 'from',\n",
       " 'th',\n",
       " 'insid',\n",
       " 'or',\n",
       " 'th',\n",
       " 'outsid',\n",
       " ',',\n",
       " 'and',\n",
       " 'devot',\n",
       " 'myself',\n",
       " 'instead',\n",
       " 'to',\n",
       " 'geograph',\n",
       " ',',\n",
       " 'histor',\n",
       " ',',\n",
       " 'arithmetic',\n",
       " ',',\n",
       " 'and',\n",
       " 'grammar',\n",
       " '.',\n",
       " 'that',\n",
       " 'i',\n",
       " 'wh',\n",
       " ',',\n",
       " 'at',\n",
       " 'th',\n",
       " 'ag',\n",
       " 'of',\n",
       " 'six',\n",
       " ',',\n",
       " 'i',\n",
       " 'gav',\n",
       " 'up',\n",
       " 'what',\n",
       " 'might',\n",
       " 'hav',\n",
       " 'been',\n",
       " 'a',\n",
       " 'magnific',\n",
       " 'career',\n",
       " 'a',\n",
       " 'a',\n",
       " 'painter',\n",
       " '.',\n",
       " 'i',\n",
       " 'had',\n",
       " 'been',\n",
       " 'dishearten',\n",
       " 'b',\n",
       " 'th',\n",
       " 'failur',\n",
       " 'of',\n",
       " 'm',\n",
       " 'draw',\n",
       " 'number',\n",
       " 'on',\n",
       " 'and',\n",
       " 'm',\n",
       " 'draw',\n",
       " 'number',\n",
       " 'two',\n",
       " '.',\n",
       " 'grown',\n",
       " '-',\n",
       " 'up',\n",
       " 'never',\n",
       " 'understand',\n",
       " 'anyth',\n",
       " 'b',\n",
       " 'themselv',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'i',\n",
       " 'tiresom',\n",
       " 'for',\n",
       " 'children',\n",
       " 'to',\n",
       " 'b',\n",
       " 'alway',\n",
       " 'and',\n",
       " 'forever',\n",
       " 'explain',\n",
       " 'thing',\n",
       " 'to',\n",
       " 'them',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EXjxQgsoK8EM"
   },
   "source": [
    "**Questions:**\n",
    "  4. What are some examples where  your stemmer on the text differs from the PorterStemmer?\n",
    "  5. Can you explain why the differences occur?\n",
    "  \n",
    "**Bonus**: Use NLTK's WordNetLemmatizer to get an array of lemmatized tokens. Where does it differ from the stemmers' outputs? Why?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\n",
    "There are many examples where my stemmer differs. \"true\", \"deepli\", \"making\" for example. For true, my stemmer removes the \"e\" at the end while their stemmer keeps it. deeply becomes deepli in their stemmer and in mine it becomes deepl and making for them is make and for me is mak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6eNguN9pNMei"
   },
   "source": [
    "5.\n",
    "The differences occur because their stemmer is a lot more complex. it recognizes the roots of words better so it has many exceptions while my stemmer is much simpler. it just removes suffixes without considering which word it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Core 1 Exercise 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
