{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxXPmYWLroVT"
   },
   "source": [
    "# NLP Core 2 Exercise 1: Hot News\n",
    "\n",
    "In this exercise we will learn how to perform document classification in order to predict the category of news articles from the Reuters Corpus using a **bag-of-words** model and **one-hot encoding**. We will then see how we can use **TF-IDF** to improve our features for classification. Finally, we will perform topic modeling with **LDA** to see whether we can predict the categories of news articles without any labelled data.\n",
    "\n",
    "## The Reuters Corpus\n",
    "\n",
    "The Reuters Corpus is a collection of news documents along with category tags that are commonly used to test document classification. It is split into two sets: the *training* documents used to train a classification algorithm, and the *test* documents used to test the classifier's performance.\n",
    "\n",
    "The Reuters Corpus is accessible through NLTK; for more information see the [NLTK Corpus HOWTO](http://www.nltk.org/howto/corpus.html#categorized-corpora).\n",
    "\n",
    "**Questions**:\n",
    "  1. How many documents are in the Reuters Corpus? What percentage are training and what percentage are testing documents?\n",
    "  2. How many words are in the training documents? In the testing documents?\n",
    "  3. What are the five most common categories in the training documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:05.878817Z",
     "start_time": "2019-08-10T14:28:00.186157Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:09.258657Z",
     "start_time": "2019-08-10T14:28:05.881826Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/Yohan/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:14.334574Z",
     "start_time": "2019-08-10T14:28:09.263618Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:14.634930Z",
     "start_time": "2019-08-10T14:28:14.338746Z"
    }
   },
   "outputs": [],
   "source": [
    "reuters_files = reuters.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:14.641573Z",
     "start_time": "2019-08-10T14:28:14.637696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10788 documents in the Reuters Corpus\n"
     ]
    }
   ],
   "source": [
    "reuters_len = len(reuters_files)\n",
    "print(\"There are {} documents in the Reuters Corpus\".format(reuters_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:14.711837Z",
     "start_time": "2019-08-10T14:28:14.644029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training % is: 72.01520207638117\n",
      "Test % is: 27.98479792361884\n"
     ]
    }
   ],
   "source": [
    "train_per = np.sum(np.array([mydoc[0:8] for mydoc in reuters_files])==\"training\")/reuters_len*100\n",
    "test_per = np.sum(np.array([mydoc[0:4] for mydoc in reuters_files])==\"test\")/reuters_len*100\n",
    "print(\"Training % is: {}\".format(train_per))\n",
    "print(\"Test % is: {}\".format(test_per))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:14.736615Z",
     "start_time": "2019-08-10T14:28:14.720426Z"
    }
   },
   "outputs": [],
   "source": [
    "training_files = np.array(reuters_files)[np.array([mydoc[0:8] for mydoc in reuters.fileids()])==\"training\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:18.675114Z",
     "start_time": "2019-08-10T14:28:14.741761Z"
    }
   },
   "outputs": [],
   "source": [
    "training_words = len([word for mydoc in training_files for word in reuters.words(mydoc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:18.699082Z",
     "start_time": "2019-08-10T14:28:18.679238Z"
    }
   },
   "outputs": [],
   "source": [
    "test_files = np.array(reuters_files)[np.array([mydoc[0:4] for mydoc in reuters.fileids()])==\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T20:19:57.738969Z",
     "start_time": "2019-08-09T20:19:57.666752Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:20.434271Z",
     "start_time": "2019-08-10T14:28:18.702215Z"
    }
   },
   "outputs": [],
   "source": [
    "test_words = len([word for mydoc in test_files for word in reuters.words(mydoc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:23.961970Z",
     "start_time": "2019-08-10T14:28:20.436260Z"
    }
   },
   "outputs": [],
   "source": [
    "train_words = len([word for mydoc in training_files for word in reuters.words(mydoc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:23.970523Z",
     "start_time": "2019-08-10T14:28:23.964689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1253696 words in the training documents\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} words in the training documents\".format(train_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:23.978718Z",
     "start_time": "2019-08-10T14:28:23.973193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 467205 words in the testing documents\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} words in the testing documents\".format(test_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:25.165601Z",
     "start_time": "2019-08-10T14:28:23.981920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "earn        1087\n",
       "acq          719\n",
       "crude        189\n",
       "money-fx     179\n",
       "grain        149\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([word for mydoc in test_files for word in reuters.categories(mydoc)]).value_counts()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MyuVHjW04jn4"
   },
   "source": [
    "## Bag of words representations\n",
    "\n",
    "We will now see how a sentence can be transformed into a feature vector using a bag of words model. Consider the following sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:25.173738Z",
     "start_time": "2019-08-10T14:28:25.168461Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Ntr9TapW-Rfl"
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  'This is the first document.',\n",
    "  'This document is the second document.',\n",
    "  'And this is the third one.',\n",
    "   'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ymyaIGW-VSC"
   },
   "source": [
    "We can represent each word as a **one-hot** encoded vector (with a single 1 in the column for that word), and add their vectors together to get the feature vector for a sentence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RiVWmYsO_xRi"
   },
   "source": [
    "**Questions:**\n",
    "  4. Use CountVectorizer from scikit-learn to get an array of one-hot encoded vectors for the given sentences. What do the rows and columns of the feature matrix X represent?\n",
    "  5. What word does the second column of X represent? What about the third column? (If you are stuck, look at *vectorizer.get_feature_names()*)\n",
    " \n",
    " **Bonus**: Try using TfidfVectorizer instead of CountVectorizer, and try to explain why some values of X become smaller than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:25.206868Z",
     "start_time": "2019-08-10T14:28:25.176197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows represent each sentence and the columns each word in the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:25.227100Z",
     "start_time": "2019-08-10T14:28:25.215676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second word in column: document\n",
      "third word in column: first\n"
     ]
    }
   ],
   "source": [
    "vector_words = vectorizer.get_feature_names()\n",
    "print(\"second word in column: {}\".format(vector_words[1]))\n",
    "print(\"third word in column: {}\".format(vector_words[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WA8pQjnp7I96"
   },
   "source": [
    "## Classifying Reuters\n",
    "\n",
    "Now let's put these together in order to build a classifier for Reuters articles.\n",
    "\n",
    "**Questions:**\n",
    "  6. Convert the training and testing documents into matrices X and X2 of feature vectors using CountVectorizer(), and convert the category labels into matrices y and y2 of binary features for classification using MultiLabelBinarizer() from scikit-learn. (Hint: use fit_transform() first on the training set, and then transform() on the testing set.)\n",
    "  7. add code to fit a multiclass SVM classifier on the training data . (Hint: use *OneVsRestClassifier(LinearSVC())* as the classifier object, and then call its fit() and predict() methods on the data.) Use sklearn.metrics.classification_report to evaluate its performance.\n",
    "  \n",
    " **Bonus**: Try using TF-IDF (TfidfVectorizer) weighted features. Does the classifier's performance improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:27.388878Z",
     "start_time": "2019-08-10T14:28:25.230240Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_docs = [reuters.raw(train_id) for train_id in training_files]\n",
    "test_docs = [reuters.raw(test_id) for test_id in test_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:29.469754Z",
     "start_time": "2019-08-10T14:28:27.391517Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 3, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 2, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### (A) add code here from question 6\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_docs)\n",
    "X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:30.445648Z",
     "start_time": "2019-08-10T14:28:29.472767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 3, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = vectorizer.transform(test_docs)\n",
    "X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:30.555581Z",
     "start_time": "2019-08-10T14:28:30.449208Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert the category labels into binary features for classification\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform([reuters.categories(train_id) for train_id in training_files])\n",
    "y2 = mlb.transform([reuters.categories(test_id) for test_id in test_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:39.856773Z",
     "start_time": "2019-08-10T14:28:30.559545Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yohan/Desktop/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "svm = OneVsRestClassifier(LinearSVC())\n",
    "y_svm = svm.fit(X_train, y)\n",
    "predictions = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T14:28:39.965847Z",
     "start_time": "2019-08-10T14:28:39.864231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       719\n",
      "           1       1.00      0.39      0.56        23\n",
      "           2       1.00      0.64      0.78        14\n",
      "           3       0.78      0.70      0.74        30\n",
      "           4       0.92      0.67      0.77        18\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       1.00      0.83      0.91        18\n",
      "           7       0.00      0.00      0.00         2\n",
      "           8       0.00      0.00      0.00         3\n",
      "           9       0.93      0.93      0.93        28\n",
      "          10       1.00      0.78      0.88        18\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       0.91      0.86      0.88        56\n",
      "          13       1.00      0.50      0.67        20\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.70      0.50      0.58        28\n",
      "          16       0.00      0.00      0.00         1\n",
      "          17       0.84      0.84      0.84       189\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.78      0.82      0.80        44\n",
      "          20       0.00      0.00      0.00         4\n",
      "          21       0.97      0.98      0.98      1087\n",
      "          22       0.67      0.20      0.31        10\n",
      "          23       1.00      0.53      0.69        17\n",
      "          24       0.90      0.80      0.85        35\n",
      "          25       0.92      0.73      0.81        30\n",
      "          26       0.90      0.81      0.86       149\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00         1\n",
      "          29       0.50      0.60      0.55         5\n",
      "          30       1.00      0.33      0.50         6\n",
      "          31       1.00      0.75      0.86         4\n",
      "          32       1.00      0.29      0.44         7\n",
      "          33       0.00      0.00      0.00         1\n",
      "          34       0.81      0.69      0.75       131\n",
      "          35       0.75      1.00      0.86        12\n",
      "          36       0.70      0.50      0.58        14\n",
      "          37       0.00      0.00      0.00         1\n",
      "          38       0.71      0.57      0.63        21\n",
      "          39       0.00      0.00      0.00         2\n",
      "          40       1.00      0.57      0.73        14\n",
      "          41       1.00      1.00      1.00         3\n",
      "          42       0.00      0.00      0.00         1\n",
      "          43       0.56      0.42      0.48        24\n",
      "          44       0.00      0.00      0.00         6\n",
      "          45       0.80      0.21      0.33        19\n",
      "          46       0.74      0.72      0.73       179\n",
      "          47       0.72      0.76      0.74        34\n",
      "          48       0.00      0.00      0.00         4\n",
      "          49       0.77      0.57      0.65        30\n",
      "          50       0.00      0.00      0.00         1\n",
      "          51       0.00      0.00      0.00         2\n",
      "          52       0.00      0.00      0.00         2\n",
      "          53       0.17      0.33      0.22         6\n",
      "          54       0.69      0.62      0.65        47\n",
      "          55       1.00      0.64      0.78        11\n",
      "          56       0.00      0.00      0.00         1\n",
      "          57       1.00      0.50      0.67        10\n",
      "          58       0.00      0.00      0.00         1\n",
      "          59       0.00      0.00      0.00        12\n",
      "          60       1.00      0.14      0.25         7\n",
      "          61       1.00      0.33      0.50         3\n",
      "          62       0.00      0.00      0.00         3\n",
      "          63       0.00      0.00      0.00         1\n",
      "          64       0.00      0.00      0.00         3\n",
      "          65       1.00      0.44      0.62         9\n",
      "          66       0.86      0.67      0.75        18\n",
      "          67       1.00      0.50      0.67         2\n",
      "          68       1.00      0.46      0.63        24\n",
      "          69       1.00      0.67      0.80        12\n",
      "          70       0.00      0.00      0.00         1\n",
      "          71       0.77      0.69      0.73        89\n",
      "          72       1.00      0.50      0.67         8\n",
      "          73       0.67      0.20      0.31        10\n",
      "          74       1.00      0.15      0.27        13\n",
      "          75       0.33      0.09      0.14        11\n",
      "          76       0.80      0.61      0.69        33\n",
      "          77       0.00      0.00      0.00        11\n",
      "          78       0.93      0.78      0.85        36\n",
      "          79       0.00      0.00      0.00         1\n",
      "          80       0.00      0.00      0.00         2\n",
      "          81       0.00      0.00      0.00         5\n",
      "          82       0.00      0.00      0.00         4\n",
      "          83       1.00      0.58      0.74        12\n",
      "          84       0.70      0.71      0.70       117\n",
      "          85       0.86      0.51      0.64        37\n",
      "          86       0.89      0.82      0.85        71\n",
      "          87       0.90      0.90      0.90        10\n",
      "          88       0.38      0.21      0.27        14\n",
      "          89       1.00      0.38      0.56        13\n",
      "\n",
      "   micro avg       0.90      0.81      0.85      3744\n",
      "   macro avg       0.56      0.39      0.44      3744\n",
      "weighted avg       0.88      0.81      0.83      3744\n",
      " samples avg       0.87      0.86      0.86      3744\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yohan/Desktop/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/Yohan/Desktop/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y2, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1APVAlin4drp"
   },
   "source": [
    "## Topic Modeling with LDA\n",
    "\n",
    "Now we will see if we can use topic modeling to discover the topics in the Reuters news articles without using the labels provided in the corpus.\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "8. Encode the articles as a matrix of feature vectors using one-hot encoding. Exclude stopwords by using NLTK's list of English stopwords (see *nltk.corpus.stopwords*).\n",
    "9. Create a model *lda* by using scikit-learn's LatentDirichletAllocation to model the topics in the documents. Set the argument *n_components* to equal the number of categories in Reuters, and use the matrix from question 8 as input to the model's *fit_transform()* function. What does the output of this function represent?\n",
    "\n",
    "**Bonus:** Plot three histograms of the most prominent topic for documents with the categories: 'trade', 'acq', 'cocoa'. (Hint: use *np.argmax(topic_matrix, axis = 1)* to find the most prominent topic for each document.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T15:42:36.712099Z",
     "start_time": "2019-08-10T15:42:31.375919Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Yohan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T16:11:48.765020Z",
     "start_time": "2019-08-10T16:11:41.992712Z"
    }
   },
   "outputs": [],
   "source": [
    "# 8.\n",
    "vectorizer = CountVectorizer(stop_words=nltk.corpus.stopwords.words('english'))\n",
    "X_train = vectorizer.fit_transform(train_docs)\n",
    "X_test = vectorizer.transform(test_docs)\n",
    "X = np.vstack([X_train.toarray(), X_test.toarray()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T16:11:55.249645Z",
     "start_time": "2019-08-10T16:11:55.241936Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T16:15:16.935187Z",
     "start_time": "2019-08-10T16:12:00.120008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.30687831e-05, 3.30687831e-05, 3.30687831e-05, ...,\n",
       "        3.30687831e-05, 3.30687831e-05, 2.77947887e-02],\n",
       "       [1.97357876e-01, 7.55857899e-05, 7.55857899e-05, ...,\n",
       "        7.55857899e-05, 7.55857899e-05, 7.55857899e-05],\n",
       "       [1.68350168e-04, 1.68350168e-04, 3.99961325e-01, ...,\n",
       "        1.68350168e-04, 1.68350168e-04, 1.68350168e-04],\n",
       "       ...,\n",
       "       [1.98412698e-04, 1.98412698e-04, 1.98412698e-04, ...,\n",
       "        1.98412698e-04, 4.33735046e-01, 1.98412698e-04],\n",
       "       [5.14403292e-05, 5.14403292e-05, 5.14403292e-05, ...,\n",
       "        2.61525055e-02, 5.14403292e-05, 5.14403292e-05],\n",
       "       [1.82149362e-04, 1.88235546e-01, 1.82149362e-04, ...,\n",
       "        1.82149362e-04, 1.82149362e-04, 1.82149362e-04]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#9.\n",
    "lda = LatentDirichletAllocation(n_components=len(reuters.categories()))\n",
    "lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output ideally represents all the categories from the reuters corpus. It represents clustering done on all the articles which should cluster all the articles according to how similar their content is."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Core 2 Exercise 1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
