{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ds2o8jo0RAah"
   },
   "source": [
    "# NLP Core 3 Exercise: Learning how to tweet from Trump\n",
    "\n",
    "In this exercise, we will train a character-level RNN language model in Keras in order to generate text, training on a dataset of tweets contributed by Donald Trump.\n",
    "\n",
    "**Note:** if you are solving this exercise in Google Colab, you must first upload the file to your runtime (use the 'Files' tab in the expanding menu to the left). Also make sure that you have the runtime type set to use GPU (in the menu *Runtime > Change runtime type*).\n",
    "\n",
    "## Loading and cleaning the data\n",
    "\n",
    "The accompanying file *trump_tweets.txt* contains a list of newline-separated Trump tweets. \n",
    "\n",
    "**Questions:**\n",
    "1. Load these tweets into a Pandas Dataframe with column 'text'.\n",
    "2. Add a new column 'cleaned' to the dataframe containing the text of the tweets with some noise cleaned -- remove URLs and replace every character that is not a basic English letter or punctuation symbol (A-Za-z.,!?@:; or a space) with the character '?'. Also feel free to clean any other kind of noise that you can find as well.\n",
    "3. Add the character '^' to the beginning of each cleaned tweet and '$' to the end.\n",
    "4. Filter the dataframe to only contain 2000 tweets between 50 and 180 letters long. Now plot a histogram of the number of characters in Trumps' tweets. What pattern do you see? (Hint: use Pandas df.str.len() and df.column.hist())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:59:03.299654Z",
     "start_time": "2019-08-11T20:58:56.835387Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:59:03.415146Z",
     "start_time": "2019-08-11T20:59:03.302451Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('trump_tweets.txt') as open_file:\n",
    "    my_lines = open_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:59:03.428003Z",
     "start_time": "2019-08-11T20:59:03.418741Z"
    }
   },
   "outputs": [],
   "source": [
    "trump_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:59:03.443202Z",
     "start_time": "2019-08-11T20:59:03.431545Z"
    }
   },
   "outputs": [],
   "source": [
    "trump_df[\"text\"] = my_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:59:03.648098Z",
     "start_time": "2019-08-11T20:59:03.447272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The race for DNC Chairman was, of course, tota...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For first time the failing @nytimes will take ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Russia talk is FAKE NEWS put out by the Dems, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Big dinner with Governors tonight at White Hou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Congressman John Lewis should spend more time ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mention crime infested) rather than falsely co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>INTELLIGENCE INSIDERS NOW CLAIM THE TRUMP DOSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Congressman John Lewis should finally focus on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Inauguration Day is turning out to be even big...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I am now going to the brand new Trump Internat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@THEREALMOGUL: 41% of American voters believe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The failing @nytimes does not mention the new ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The last person corrupt Hillary Clinton wants ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Hillary Clinton does not have the STRENGTH or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@ray_aub:  @nytimes @CNN People forget the pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@ssheaver: @realDonaldTrump @nytimes @CNN http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@mysteriousLoser: @realDonaldTrump did you hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I told you @TIME Magazine would never pick me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I will be interviewed on @foxandfriends at 7:0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@TimeHasCome1: @WayneDupreeShow @ThePatriot143...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>@autoprofessor17: @realDonaldTrump Great job o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>@WVTTS1017: @realDonaldTrump just listened to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>@AnnesLimo: @realDonaldTrump @WVTTS1017 Thanks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Big wins in West Virginia and Nebraska. Get re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I don't want to hit Crazy Bernie Sanders too h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Thank you to Donald Rumsfeld for the endorseme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Hillary Clinton may be the most corrupt person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ISIS threatens us today because of the decisio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The Bernie Sanders supporters are furious with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Tim Kaine is, and always has been, owned by th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517</th>\n",
       "      <td>Totally biased @NBCNews went out of its way to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2518</th>\n",
       "      <td>to the U.S., but had nothing to do with TRUMP,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2519</th>\n",
       "      <td>Bayer AG has pledged to add U.S. jobs and inve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2520</th>\n",
       "      <td>No wonder the Today Show on biased @NBC is doi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2521</th>\n",
       "      <td>.@TheAlabamaBand was great last night in D.C. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2522</th>\n",
       "      <td>Crime is out of control, and rapidly getting w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2523</th>\n",
       "      <td>Bernie Sanders, who has lost most of his lever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2524</th>\n",
       "      <td>I am somewhat surprised that Bernie Sanders wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>Really bad shooting in Orlando. Police investi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2526</th>\n",
       "      <td>Clinton made a false ad about me where I was i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2527</th>\n",
       "      <td>Reporting that Orlando killer shouted \"Allah h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2528</th>\n",
       "      <td>@WandaWalls20: @realDonaldTrump Please make us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2529</th>\n",
       "      <td>742096033207844864\\trealDonaldTrump\\tWhat has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2530</th>\n",
       "      <td>673098948479987712\\trealDonaldTrump\\t@American...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2531</th>\n",
       "      <td>Raleigh, North Carolina, was fantastic last ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>@Pimpburgh2015: @KatyTurNBC @realDonaldTrump j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533</th>\n",
       "      <td>Wonder if Obama will ever say RADICAL ISLAMIC ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534</th>\n",
       "      <td>I said that Crooked Hillary Clinton is \"not qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2535</th>\n",
       "      <td>Crooked Hillary has zero imagination and even ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>Look where the world is today, a total mess, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>Crooked Hillary Clinton looks presidential? I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2538</th>\n",
       "      <td>I will be interviewed on @Morning_Joe at 6:30 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539</th>\n",
       "      <td>Crooked Hillary can't even close the deal with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>Will be interviewed by @foxandfriends at 7:30 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541</th>\n",
       "      <td>@montgomeriefdn: @TrumpTurnberry Best Links Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2542</th>\n",
       "      <td>@montgomeriefdn  Colin, great to have you at T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2543</th>\n",
       "      <td>Thank you @TheTodaysGolfer for the wonderful s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2544</th>\n",
       "      <td>Crooked Hillary Clinton wants to essentially a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2545</th>\n",
       "      <td>Crooked Hillary is spending tremendous amounts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2546</th>\n",
       "      <td>Ted Cruz is mathematically out of winning the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2547 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0     The race for DNC Chairman was, of course, tota...\n",
       "1     For first time the failing @nytimes will take ...\n",
       "2     Russia talk is FAKE NEWS put out by the Dems, ...\n",
       "3     Big dinner with Governors tonight at White Hou...\n",
       "4     Congressman John Lewis should spend more time ...\n",
       "5     mention crime infested) rather than falsely co...\n",
       "6     INTELLIGENCE INSIDERS NOW CLAIM THE TRUMP DOSS...\n",
       "7     Congressman John Lewis should finally focus on...\n",
       "8     Inauguration Day is turning out to be even big...\n",
       "9     I am now going to the brand new Trump Internat...\n",
       "10    @THEREALMOGUL: 41% of American voters believe ...\n",
       "11    The failing @nytimes does not mention the new ...\n",
       "12    The last person corrupt Hillary Clinton wants ...\n",
       "13    Hillary Clinton does not have the STRENGTH or ...\n",
       "14    @ray_aub:  @nytimes @CNN People forget the pol...\n",
       "15    @ssheaver: @realDonaldTrump @nytimes @CNN http...\n",
       "16    @mysteriousLoser: @realDonaldTrump did you hea...\n",
       "17    I told you @TIME Magazine would never pick me ...\n",
       "18    I will be interviewed on @foxandfriends at 7:0...\n",
       "19    @TimeHasCome1: @WayneDupreeShow @ThePatriot143...\n",
       "20    @autoprofessor17: @realDonaldTrump Great job o...\n",
       "21    @WVTTS1017: @realDonaldTrump just listened to ...\n",
       "22    @AnnesLimo: @realDonaldTrump @WVTTS1017 Thanks...\n",
       "23    Big wins in West Virginia and Nebraska. Get re...\n",
       "24    I don't want to hit Crazy Bernie Sanders too h...\n",
       "25    Thank you to Donald Rumsfeld for the endorseme...\n",
       "26    Hillary Clinton may be the most corrupt person...\n",
       "27    ISIS threatens us today because of the decisio...\n",
       "28    The Bernie Sanders supporters are furious with...\n",
       "29    Tim Kaine is, and always has been, owned by th...\n",
       "...                                                 ...\n",
       "2517  Totally biased @NBCNews went out of its way to...\n",
       "2518  to the U.S., but had nothing to do with TRUMP,...\n",
       "2519  Bayer AG has pledged to add U.S. jobs and inve...\n",
       "2520  No wonder the Today Show on biased @NBC is doi...\n",
       "2521  .@TheAlabamaBand was great last night in D.C. ...\n",
       "2522  Crime is out of control, and rapidly getting w...\n",
       "2523  Bernie Sanders, who has lost most of his lever...\n",
       "2524  I am somewhat surprised that Bernie Sanders wa...\n",
       "2525  Really bad shooting in Orlando. Police investi...\n",
       "2526  Clinton made a false ad about me where I was i...\n",
       "2527  Reporting that Orlando killer shouted \"Allah h...\n",
       "2528  @WandaWalls20: @realDonaldTrump Please make us...\n",
       "2529  742096033207844864\\trealDonaldTrump\\tWhat has ...\n",
       "2530  673098948479987712\\trealDonaldTrump\\t@American...\n",
       "2531  Raleigh, North Carolina, was fantastic last ni...\n",
       "2532  @Pimpburgh2015: @KatyTurNBC @realDonaldTrump j...\n",
       "2533  Wonder if Obama will ever say RADICAL ISLAMIC ...\n",
       "2534  I said that Crooked Hillary Clinton is \"not qu...\n",
       "2535  Crooked Hillary has zero imagination and even ...\n",
       "2536  Look where the world is today, a total mess, a...\n",
       "2537  Crooked Hillary Clinton looks presidential? I ...\n",
       "2538  I will be interviewed on @Morning_Joe at 6:30 ...\n",
       "2539  Crooked Hillary can't even close the deal with...\n",
       "2540  Will be interviewed by @foxandfriends at 7:30 ...\n",
       "2541  @montgomeriefdn: @TrumpTurnberry Best Links Co...\n",
       "2542  @montgomeriefdn  Colin, great to have you at T...\n",
       "2543  Thank you @TheTodaysGolfer for the wonderful s...\n",
       "2544  Crooked Hillary Clinton wants to essentially a...\n",
       "2545  Crooked Hillary is spending tremendous amounts...\n",
       "2546  Ted Cruz is mathematically out of winning the ...\n",
       "\n",
       "[2547 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:59:03.889978Z",
     "start_time": "2019-08-11T20:59:03.653444Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "trump_df['cleaned'] = trump_df['text'].str.replace(\"http\\S+\", \"\").str.replace('[^A-Za-z\\.,!\\?@:; ]','?')\n",
    "trump_df['cleaned'] = \"^\" + trump_df['cleaned'] + \"$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:59:03.911469Z",
     "start_time": "2019-08-11T20:59:03.892786Z"
    }
   },
   "outputs": [],
   "source": [
    "len_vec = trump_df.cleaned.apply(len)\n",
    "trump_df = trump_df[(len_vec>=50) & (len_vec <= 180)]\n",
    "trump_df = trump_df.sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:59:03.933263Z",
     "start_time": "2019-08-11T20:59:03.915140Z"
    }
   },
   "outputs": [],
   "source": [
    "len_vec_2 = trump_df.cleaned.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:59:05.640230Z",
     "start_time": "2019-08-11T20:59:03.941149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11f9fffd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_vec_2.hist(bins = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that trump's tweets centered around 144 which used to be the maximmu number of tweets until they changed it to 280."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mG6wEHukN5Qw"
   },
   "source": [
    "## Computing feature vectors\n",
    "\n",
    "We will now convert the tweets into feature vectors that can be used to train a network.\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "5. Create a variable *charset* containing a string with all of the unique characters used in the cleaned tweets, and with the padding character '0' at the beginning (so it should have a value similar to '0 !$,.:;?@ABCDEFGHIJKLMNOPQRSTUVWXYZ^abcdefghijklmnopqrstuvwxyz').\n",
    "6. Convert the tweets to vectors of character indices (starting at 1). Hint: use the function charset.index().\n",
    "7. Use the function *pad_sequences()* from keras.preprocessing.sequence to make each feature vector of length 200, by adding zeros at the end of the vectors (use the attributes value = 0, padding = 'post', maxlen = 200). Save the output matrix of feature vectors in a numpy array *data*. Each row of *data* should be the feature vector for one tweet. What is the shape of *data*?\n",
    "8. Use *data* to generate input and target matrices *X* and *Y*. X should be a matrix of character indices of shape (2000, 199). *Y* should be a one-hot encoded tensor of shape (2000, 199, #) for some number # (use *to_categorical()* from keras.utils to one-hot encode Y). Note: Y should be offset one character from X since we want to predict the next character in a string given what came before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:59:05.651637Z",
     "start_time": "2019-08-11T20:59:05.642379Z"
    }
   },
   "outputs": [],
   "source": [
    "charset = set(trump_df['cleaned'].str.cat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:59:05.659631Z",
     "start_time": "2019-08-11T20:59:05.655988Z"
    }
   },
   "outputs": [],
   "source": [
    "charset = '0' + \"\".join(charset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:59:05.767351Z",
     "start_time": "2019-08-11T20:59:05.663473Z"
    }
   },
   "outputs": [],
   "source": [
    "trump_vectors = trump_df['cleaned'].apply(lambda x: [charset.index(c) for c in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:59:29.503754Z",
     "start_time": "2019-08-11T20:59:05.769335Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:59:29.556162Z",
     "start_time": "2019-08-11T20:59:29.506220Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pad_sequences(trump_vectors.to_list(), value = 0, padding = 'post', maxlen = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:59:29.562927Z",
     "start_time": "2019-08-11T20:59:29.558372Z"
    }
   },
   "outputs": [],
   "source": [
    "X = data[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:59:29.568893Z",
     "start_time": "2019-08-11T20:59:29.565336Z"
    }
   },
   "outputs": [],
   "source": [
    "y = data[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:59:29.580829Z",
     "start_time": "2019-08-11T20:59:29.574001Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:59:29.822087Z",
     "start_time": "2019-08-11T20:59:29.586136Z"
    }
   },
   "outputs": [],
   "source": [
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yhNngjdSbakM"
   },
   "source": [
    "## Building and training the language model\n",
    "\n",
    "We will start by using the following imports and hyperparameter settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:59:29.830490Z",
     "start_time": "2019-08-11T20:59:29.825189Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "eKHa2qE6N3no"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, LSTM, Embedding, TimeDistributed\n",
    "hidden_size = 128\n",
    "embedding_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GaaiMqB2dqX2"
   },
   "source": [
    "We will now build a character-level RNN language model that can learn from the given data.\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "9.  Build a sequential model *model*, and using *model.add()* add the following four layers:\n",
    "  * Embedding layer with output dimension *embedding_size*. Use mask_zero = True since we zero-padded the input, and set input_length and input_dim to match the dimensions of X and the number of possible values that features in X can take.\n",
    "  * LSTM layer with hidden state dimension *hidden_size*. Use return_sequences = True to make the layer output the sequence of hidden states.\n",
    "  * Fully-connected layer -- use *TimeDistributed(Dense(#))* for some number #\n",
    "  * Softmax activation layer\n",
    "  \n",
    "10. Compile the model (*model.compile()*) with loss function 'categorical_crossentropy' and optimizer 'adam'. Examine the output shapes of the model's layers with *model.summary()*. How do you interpret the output of the final layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:59:30.794479Z",
     "start_time": "2019-08-11T20:59:29.834175Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0811 23:59:29.838676 140735838831488 deprecation_wrapper.py:119] From /Users/Yohan/Desktop/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0811 23:59:29.916720 140735838831488 deprecation_wrapper.py:119] From /Users/Yohan/Desktop/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0811 23:59:29.943813 140735838831488 deprecation_wrapper.py:119] From /Users/Yohan/Desktop/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0811 23:59:30.582655 140735838831488 deprecation.py:323] From /Users/Yohan/Desktop/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0811 23:59:30.687045 140735838831488 deprecation_wrapper.py:119] From /Users/Yohan/Desktop/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0811 23:59:30.751199 140735838831488 deprecation_wrapper.py:119] From /Users/Yohan/Desktop/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 199, 8)            504       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 199, 128)          70144     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 199, 63)           8127      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 199, 63)           0         \n",
      "=================================================================\n",
      "Total params: 78,775\n",
      "Trainable params: 78,775\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(charset)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(mask_zero = True, input_dim = vocab_size, output_dim=embedding_size, input_length=199))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(vocab_size)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYTBJGrhcMs_"
   },
   "source": [
    "##  Generating tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CTe5eB1qhY1e"
   },
   "source": [
    "Now we will train our language model and use it to generate tweets.\n",
    "\n",
    "We can generate a tweet using our model as follows:\n",
    "* Start with the beginning-of-string token '^' as the initial input.\n",
    "* Predict the distribution of the next character using the model and use the distribution to select the next character in the tweet (can use *np.random.choice()* for this)\n",
    "* Repeat the process until the end-of-string token '$' is predicted or until 200 characters are generated\n",
    "\n",
    "**Questions:**\n",
    "11. Make a function *generate_tweet()* that returns the string of a tweet generated by the model, using the above procedure. What does its output look like (before the model is trained)?\n",
    "12. Train the model using *model.fit*, with batch_size = 128 and validation_split = 0.2, for 100 epochs. Generate a tweet using the model every 20 epochs. What do you see?\n",
    "13. Keep training the model as long as it is underfitting (or until you get bored) and observe how the model learns to generate better tweets.\n",
    "\n",
    "**Bonus exercises:**\n",
    "* Try changing the model hyperparameters (embedding and hidden dimensions, batch size). How does this affect the learning rate and/or output?\n",
    "* Add a temperature parameter T in the generation step by adding a Lambda layer before the softmax layer with lambda x: x / temp. How do you expect this would affect output, and why?\n",
    "* Use https://faketrumptweet.com/ to fool your family and friends with your best randomly-generated Trump tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T21:01:32.951966Z",
     "start_time": "2019-08-11T21:01:32.378224Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0812 00:01:32.396860 140735838831488 deprecation_wrapper.py:119] From /Users/Yohan/Desktop/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'^$'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#11\n",
    "maxlen=200\n",
    "import numpy as np\n",
    "def generate_tweet():\n",
    "    indices = [charset.index('^')]\n",
    "    while len(indices) < maxlen:\n",
    "        X = pad_sequences([indices], maxlen=maxlen-1, padding=\"post\", value=0)\n",
    "        next_word_dist = model.predict(X)[0,-1,:]\n",
    "        next_word = np.random.choice(len(next_word_dist), p=next_word_dist)\n",
    "        indices.append(next_word)\n",
    "        if next_word == charset.index('$'):\n",
    "            break\n",
    "    return ''.join(charset[i] for i in indices)\n",
    "generate_tweet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T21:01:45.591347Z",
     "start_time": "2019-08-11T21:01:45.586879Z"
    }
   },
   "outputs": [],
   "source": [
    "epoch,epochs_per_iteration=0,20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-12T04:15:55.288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/20\n",
      "1600/1600 [==============================] - 16s 10ms/step - loss: 1.4094 - val_loss: 1.6224\n",
      "Epoch 2/20\n",
      "1600/1600 [==============================] - 15s 9ms/step - loss: 1.4089 - val_loss: 1.6216\n",
      "Epoch 3/20\n",
      "1600/1600 [==============================] - 18s 11ms/step - loss: 1.4071 - val_loss: 1.6221\n",
      "Epoch 4/20\n",
      "1600/1600 [==============================] - 18s 11ms/step - loss: 1.4056 - val_loss: 1.6215\n",
      "Epoch 5/20\n",
      "1600/1600 [==============================] - 16s 10ms/step - loss: 1.4047 - val_loss: 1.6221\n",
      "Epoch 6/20\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 1.4048 - val_loss: 1.6208\n",
      "Epoch 7/20\n",
      "1600/1600 [==============================] - 17s 11ms/step - loss: 1.4040 - val_loss: 1.6218\n",
      "Epoch 8/20\n",
      "1600/1600 [==============================] - 25s 16ms/step - loss: 1.4028 - val_loss: 1.6212\n",
      "Epoch 9/20\n",
      "1600/1600 [==============================] - 21s 13ms/step - loss: 1.4019 - val_loss: 1.6225\n",
      "Epoch 10/20\n",
      "1600/1600 [==============================] - 19s 12ms/step - loss: 1.4011 - val_loss: 1.6211\n",
      "Epoch 11/20\n",
      "1600/1600 [==============================] - 20s 12ms/step - loss: 1.4003 - val_loss: 1.6220\n",
      "Epoch 12/20\n",
      "1600/1600 [==============================] - 15s 9ms/step - loss: 1.3995 - val_loss: 1.6219\n",
      "Epoch 13/20\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 1.3987 - val_loss: 1.6215\n",
      "Epoch 14/20\n",
      "1600/1600 [==============================] - 20s 13ms/step - loss: 1.3974 - val_loss: 1.6213\n",
      "Epoch 15/20\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 1.3970 - val_loss: 1.6213\n",
      "Epoch 16/20\n",
      "1600/1600 [==============================] - 15s 10ms/step - loss: 1.3962 - val_loss: 1.6211\n",
      "Epoch 17/20\n",
      "1600/1600 [==============================] - 15s 9ms/step - loss: 1.3950 - val_loss: 1.6218\n",
      "Epoch 18/20\n",
      "1600/1600 [==============================] - 15s 9ms/step - loss: 1.3948 - val_loss: 1.6216\n",
      "Epoch 19/20\n",
      "1600/1600 [==============================] - 17s 10ms/step - loss: 1.3936 - val_loss: 1.6216\n",
      "Epoch 20/20\n",
      "1600/1600 [==============================] - 19s 12ms/step - loss: 1.3930 - val_loss: 1.6212\n",
      "Result of 420-th epoch:\n",
      "^@larleveilllig:  @petmlase @ReCYoshe Couls Jot halls in tirnaby, ? polls of The Carson alvess ROP on @No newtupply trithon runiot Mirson phing tominament? Digg...?$\n",
      "\n",
      "\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/20\n",
      "1600/1600 [==============================] - 22s 13ms/step - loss: 1.3921 - val_loss: 1.6220\n",
      "Epoch 2/20\n",
      "1600/1600 [==============================] - 15s 10ms/step - loss: 1.3920 - val_loss: 1.6221\n",
      "Epoch 3/20\n",
      "1600/1600 [==============================] - 21s 13ms/step - loss: 1.3912 - val_loss: 1.6215\n",
      "Epoch 4/20\n",
      "1600/1600 [==============================] - 25s 15ms/step - loss: 1.3897 - val_loss: 1.6217\n",
      "Epoch 5/20\n",
      "1600/1600 [==============================] - 23s 15ms/step - loss: 1.3889 - val_loss: 1.6209\n",
      "Epoch 6/20\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 1.3878 - val_loss: 1.6209\n",
      "Epoch 7/20\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 1.3876 - val_loss: 1.6206\n",
      "Epoch 8/20\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 1.3867 - val_loss: 1.6207\n",
      "Epoch 9/20\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 1.3858 - val_loss: 1.6214\n",
      "Epoch 10/20\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 1.3849 - val_loss: 1.6216\n",
      "Epoch 11/20\n",
      "1600/1600 [==============================] - 16s 10ms/step - loss: 1.3840 - val_loss: 1.6198\n",
      "Epoch 12/20\n",
      "1600/1600 [==============================] - 18s 11ms/step - loss: 1.3836 - val_loss: 1.6219\n",
      "Epoch 13/20\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 1.3828 - val_loss: 1.6213\n",
      "Epoch 14/20\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 1.3823 - val_loss: 1.6214\n",
      "Epoch 15/20\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 1.3812 - val_loss: 1.6203\n",
      "Epoch 16/20\n",
      "1600/1600 [==============================] - 17s 11ms/step - loss: 1.3805 - val_loss: 1.6214\n",
      "Epoch 17/20\n",
      "1600/1600 [==============================] - 19s 12ms/step - loss: 1.3800 - val_loss: 1.6211\n",
      "Epoch 18/20\n",
      "1600/1600 [==============================] - 15s 9ms/step - loss: 1.3789 - val_loss: 1.6216\n",
      "Epoch 19/20\n",
      "1600/1600 [==============================] - 19s 12ms/step - loss: 1.3782 - val_loss: 1.6230\n",
      "Epoch 20/20\n",
      "1600/1600 [==============================] - 16s 10ms/step - loss: 1.3773 - val_loss: 1.6218\n",
      "Result of 440-th epoch:\n",
      "^Tere crazy, in Colmonst kipt Othirrat mor in Rissine forwing you at ?:?? P.M. jush sunestist are good to wlrefh, can?t on you!?$\n",
      "\n",
      "\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/20\n",
      "1600/1600 [==============================] - 16s 10ms/step - loss: 1.3776 - val_loss: 1.6467\n",
      "Epoch 2/20\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 1.3905 - val_loss: 1.6228\n",
      "Epoch 3/20\n",
      "1600/1600 [==============================] - 15s 9ms/step - loss: 1.3797 - val_loss: 1.6207\n",
      "Epoch 4/20\n",
      "1600/1600 [==============================] - 16s 10ms/step - loss: 1.3767 - val_loss: 1.6199\n",
      "Epoch 5/20\n",
      "1600/1600 [==============================] - 17s 10ms/step - loss: 1.3746 - val_loss: 1.6204\n",
      "Epoch 6/20\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 1.3735 - val_loss: 1.6189\n",
      "Epoch 7/20\n",
      "1600/1600 [==============================] - 16s 10ms/step - loss: 1.3736 - val_loss: 1.6213\n",
      "Epoch 8/20\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 1.3722 - val_loss: 1.6213\n",
      "Epoch 9/20\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 1.3709 - val_loss: 1.6211\n",
      "Epoch 10/20\n",
      "1600/1600 [==============================] - 14s 8ms/step - loss: 1.3709 - val_loss: 1.6228\n",
      "Epoch 11/20\n",
      "1600/1600 [==============================] - 15s 10ms/step - loss: 1.3695 - val_loss: 1.6205\n",
      "Epoch 12/20\n",
      "1600/1600 [==============================] - 16s 10ms/step - loss: 1.3683 - val_loss: 1.6224\n",
      "Epoch 13/20\n",
      "1600/1600 [==============================] - 17s 11ms/step - loss: 1.3681 - val_loss: 1.6213\n",
      "Epoch 14/20\n",
      "1600/1600 [==============================] - 16s 10ms/step - loss: 1.3674 - val_loss: 1.6219\n",
      "Epoch 15/20\n",
      "1600/1600 [==============================] - 14s 8ms/step - loss: 1.3667 - val_loss: 1.6227\n",
      "Epoch 16/20\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 1.3657 - val_loss: 1.6223\n",
      "Epoch 17/20\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 1.3650 - val_loss: 1.6216\n",
      "Epoch 18/20\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 1.3645 - val_loss: 1.6216\n",
      "Epoch 19/20\n",
      "1600/1600 [==============================] - 18s 11ms/step - loss: 1.3634 - val_loss: 1.6219\n",
      "Epoch 20/20\n",
      "1600/1600 [==============================] - 19s 12ms/step - loss: 1.3634 - val_loss: 1.6225\n",
      "Result of 460-th epoch:\n",
      "^???????????????????realDonaldTrump?@voxalce: Trump is lost the publents fur to relieved but the ratings out consine for Trump on Mesects!  ?ROSI with TIr Hantubublis?!??$\n",
      "\n",
      "\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/20\n",
      "1600/1600 [==============================] - 17s 11ms/step - loss: 1.3628 - val_loss: 1.6236\n",
      "Epoch 2/20\n",
      "1600/1600 [==============================] - 18s 12ms/step - loss: 1.3620 - val_loss: 1.6217\n",
      "Epoch 3/20\n",
      "1600/1600 [==============================] - 16s 10ms/step - loss: 1.3613 - val_loss: 1.6225\n",
      "Epoch 4/20\n",
      "1600/1600 [==============================] - 19s 12ms/step - loss: 1.3605 - val_loss: 1.6239\n",
      "Epoch 5/20\n",
      "1600/1600 [==============================] - 18s 12ms/step - loss: 1.3597 - val_loss: 1.6243\n",
      "Epoch 6/20\n",
      "1600/1600 [==============================] - 24s 15ms/step - loss: 1.3596 - val_loss: 1.6230\n",
      "Epoch 7/20\n",
      "1600/1600 [==============================] - 21s 13ms/step - loss: 1.3593 - val_loss: 1.6241\n",
      "Epoch 8/20\n",
      "1600/1600 [==============================] - 20s 12ms/step - loss: 1.3578 - val_loss: 1.6226\n",
      "Epoch 9/20\n",
      "1600/1600 [==============================] - 24s 15ms/step - loss: 1.3567 - val_loss: 1.6245\n",
      "Epoch 10/20\n",
      "1600/1600 [==============================] - 16s 10ms/step - loss: 1.3564 - val_loss: 1.6234\n",
      "Epoch 11/20\n",
      "1600/1600 [==============================] - 18s 11ms/step - loss: 1.3554 - val_loss: 1.6246\n",
      "Epoch 12/20\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 1.3548 - val_loss: 1.6246\n",
      "Epoch 13/20\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 1.3539 - val_loss: 1.6238\n",
      "Epoch 14/20\n",
      "1600/1600 [==============================] - 16s 10ms/step - loss: 1.3541 - val_loss: 1.6240\n",
      "Epoch 15/20\n",
      "1600/1600 [==============================] - 20s 12ms/step - loss: 1.3526 - val_loss: 1.6258\n",
      "Epoch 16/20\n",
      "1600/1600 [==============================] - 17s 11ms/step - loss: 1.3532 - val_loss: 1.6244\n",
      "Epoch 17/20\n",
      "1600/1600 [==============================] - 16s 10ms/step - loss: 1.3531 - val_loss: 1.6237\n",
      "Epoch 18/20\n",
      "1600/1600 [==============================] - 20s 12ms/step - loss: 1.3519 - val_loss: 1.6250\n",
      "Epoch 19/20\n",
      "1600/1600 [==============================] - 16s 10ms/step - loss: 1.3508 - val_loss: 1.6249\n",
      "Epoch 20/20\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 1.3511 - val_loss: 1.6243\n",
      "Result of 480-th epoch:\n",
      "^I will be interviewed  Things, news hatch a Hinderer? Fincall proby kighly and I have geesing another fod wrut. When a speening in votes. Y? ever!?$\n",
      "\n",
      "\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/20\n",
      "1600/1600 [==============================] - 15s 10ms/step - loss: 1.3496 - val_loss: 1.6251\n",
      "Epoch 2/20\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 1.3487 - val_loss: 1.6247\n",
      "Epoch 3/20\n",
      "1600/1600 [==============================] - 15s 9ms/step - loss: 1.3479 - val_loss: 1.6254\n",
      "Epoch 4/20\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 1.3470 - val_loss: 1.6263\n",
      "Epoch 5/20\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 1.3466 - val_loss: 1.6245\n",
      "Epoch 6/20\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 1.3455 - val_loss: 1.6265\n",
      "Epoch 7/20\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 1.3458 - val_loss: 1.6256\n",
      "Epoch 8/20\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 1.3451 - val_loss: 1.6260\n",
      "Epoch 9/20\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 1.3447 - val_loss: 1.6283\n",
      "Epoch 10/20\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 1.3434 - val_loss: 1.6272\n",
      "Epoch 11/20\n",
      "1600/1600 [==============================] - 23s 14ms/step - loss: 1.3433 - val_loss: 1.6279\n",
      "Epoch 12/20\n",
      "1600/1600 [==============================] - 20s 13ms/step - loss: 1.3424 - val_loss: 1.6266\n",
      "Epoch 13/20\n",
      "1600/1600 [==============================] - 17s 11ms/step - loss: 1.3412 - val_loss: 1.6265\n",
      "Epoch 14/20\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 1.3408 - val_loss: 1.6275\n",
      "Epoch 15/20\n",
      "1600/1600 [==============================] - 17s 10ms/step - loss: 1.3403 - val_loss: 1.6284\n",
      "Epoch 16/20\n",
      "1600/1600 [==============================] - 6126s 4s/step - loss: 1.3399 - val_loss: 1.6290\n",
      "Epoch 17/20\n",
      "1600/1600 [==============================] - 29s 18ms/step - loss: 1.3389 - val_loss: 1.6273\n",
      "Epoch 18/20\n",
      "1600/1600 [==============================] - 28s 17ms/step - loss: 1.3380 - val_loss: 1.6270\n",
      "Epoch 19/20\n",
      "1600/1600 [==============================] - 15s 9ms/step - loss: 1.3374 - val_loss: 1.6274\n",
      "Epoch 20/20\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 1.3369 - val_loss: 1.6292\n",
      "Result of 500-th epoch:\n",
      "^The Republican the neliese thro shooning on my run good by crime?. They would say tonatible to ?camery wondeng them o?? no truzy??$\n",
      "\n",
      "\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/20\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 1.3360 - val_loss: 1.6293\n",
      "Epoch 2/20\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 1.3357 - val_loss: 1.6286\n",
      "Epoch 3/20\n",
      "1600/1600 [==============================] - 14s 8ms/step - loss: 1.3353 - val_loss: 1.6296\n",
      "Epoch 4/20\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 1.3351 - val_loss: 1.6289\n",
      "Epoch 5/20\n",
      "1600/1600 [==============================] - 14s 8ms/step - loss: 1.3345 - val_loss: 1.6292\n",
      "Epoch 6/20\n",
      "1600/1600 [==============================] - 12s 8ms/step - loss: 1.3335 - val_loss: 1.6302\n",
      "Epoch 7/20\n",
      "1600/1600 [==============================] - 16s 10ms/step - loss: 1.3334 - val_loss: 1.6285\n",
      "Epoch 8/20\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 1.3324 - val_loss: 1.6284\n",
      "Epoch 9/20\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 1.3317 - val_loss: 1.6296\n",
      "Epoch 10/20\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 1.3310 - val_loss: 1.6290\n",
      "Epoch 11/20\n",
      "1600/1600 [==============================] - 15s 9ms/step - loss: 1.3302 - val_loss: 1.6316\n",
      "Epoch 12/20\n",
      "1600/1600 [==============================] - 17s 11ms/step - loss: 1.3306 - val_loss: 1.6303\n",
      "Epoch 13/20\n",
      "1600/1600 [==============================] - 14s 8ms/step - loss: 1.3294 - val_loss: 1.6300\n",
      "Epoch 14/20\n",
      "1600/1600 [==============================] - 17s 10ms/step - loss: 1.3287 - val_loss: 1.6310\n",
      "Epoch 15/20\n",
      "1600/1600 [==============================] - 16s 10ms/step - loss: 1.3278 - val_loss: 1.6312\n",
      "Epoch 16/20\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 1.3272 - val_loss: 1.6317\n",
      "Epoch 17/20\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 1.3268 - val_loss: 1.6305\n",
      "Epoch 18/20\n",
      "1600/1600 [==============================] - 12s 7ms/step - loss: 1.3260 - val_loss: 1.6311\n",
      "Epoch 19/20\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 1.3259 - val_loss: 1.6317\n",
      "Epoch 20/20\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 1.3251 - val_loss: 1.6309\n",
      "Result of 520-th epoch:\n",
      "^The Baky Cittict questions dead Borson @Cruzznne way to where just I am Bill far speak. FTrim. Masy took people hainh Poll speeck!?$\n",
      "\n",
      "\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/20\n",
      "1600/1600 [==============================] - 16s 10ms/step - loss: 1.3240 - val_loss: 1.6312\n",
      "Epoch 2/20\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 1.3237 - val_loss: 1.6317\n",
      "Epoch 3/20\n",
      "1600/1600 [==============================] - 15s 9ms/step - loss: 1.3226 - val_loss: 1.6318\n",
      "Epoch 4/20\n",
      "1600/1600 [==============================] - 15s 10ms/step - loss: 1.3225 - val_loss: 1.6335\n",
      "Epoch 5/20\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 1.3226 - val_loss: 1.6315\n",
      "Epoch 6/20\n",
      "1600/1600 [==============================] - 16s 10ms/step - loss: 1.3215 - val_loss: 1.6324\n",
      "Epoch 7/20\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 1.3216 - val_loss: 1.6331\n",
      "Epoch 8/20\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 1.3202 - val_loss: 1.6350\n",
      "Epoch 9/20\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 1.3203 - val_loss: 1.6355\n",
      "Epoch 10/20\n",
      "1600/1600 [==============================] - 12s 7ms/step - loss: 1.3209 - val_loss: 1.6332\n",
      "Epoch 11/20\n",
      "1600/1600 [==============================] - 12s 8ms/step - loss: 1.3194 - val_loss: 1.6318\n",
      "Epoch 12/20\n",
      "1600/1600 [==============================] - 12s 8ms/step - loss: 1.3193 - val_loss: 1.6333\n",
      "Epoch 13/20\n",
      "1600/1600 [==============================] - 14s 8ms/step - loss: 1.3185 - val_loss: 1.6331\n",
      "Epoch 14/20\n",
      "1600/1600 [==============================] - 10s 7ms/step - loss: 1.3170 - val_loss: 1.6338\n",
      "Epoch 15/20\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 1.3162 - val_loss: 1.6354\n",
      "Epoch 16/20\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 1.3156 - val_loss: 1.6353\n",
      "Epoch 17/20\n",
      "1600/1600 [==============================] - 15s 9ms/step - loss: 1.3162 - val_loss: 1.6344\n",
      "Epoch 18/20\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 1.3158 - val_loss: 1.6345\n",
      "Epoch 19/20\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 1.3144 - val_loss: 1.6345\n",
      "Epoch 20/20\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 1.3132 - val_loss: 1.6359\n",
      "Result of 540-th epoch:\n",
      "^Hirlion Crooked Hiller Cristed on our crowds and all debetiden?wan it distyne?s truttly incruries, a spiend Stotes!?$\n",
      "\n",
      "\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/20\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 1.3131 - val_loss: 1.6365\n",
      "Epoch 2/20\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 1.3128 - val_loss: 1.6359\n",
      "Epoch 3/20\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 1.3115 - val_loss: 1.6362\n",
      "Epoch 4/20\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 1.3113 - val_loss: 1.6369\n",
      "Epoch 5/20\n",
      "1600/1600 [==============================] - 18s 11ms/step - loss: 1.3108 - val_loss: 1.6352\n",
      "Epoch 6/20\n",
      "1600/1600 [==============================] - 10s 7ms/step - loss: 1.3102 - val_loss: 1.6379\n",
      "Epoch 7/20\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 1.3105 - val_loss: 1.6371\n",
      "Epoch 8/20\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 1.3104 - val_loss: 1.6414\n",
      "Epoch 9/20\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 1.3103 - val_loss: 1.6372\n",
      "Epoch 10/20\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 1.3090 - val_loss: 1.6391\n",
      "Epoch 11/20\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 1.3084 - val_loss: 1.6365\n",
      "Epoch 12/20\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 1.3077 - val_loss: 1.6384\n",
      "Epoch 13/20\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 1.3063 - val_loss: 1.6391\n",
      "Epoch 14/20\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 1.3063 - val_loss: 1.6378\n",
      "Epoch 15/20\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 1.3057 - val_loss: 1.6392\n",
      "Epoch 16/20\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 1.3057 - val_loss: 1.6377\n",
      "Epoch 17/20\n",
      "1600/1600 [==============================] - 12s 7ms/step - loss: 1.3049 - val_loss: 1.6386\n",
      "Epoch 18/20\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 1.3043 - val_loss: 1.6385\n",
      "Epoch 19/20\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 1.3039 - val_loss: 1.6403\n",
      "Epoch 20/20\n",
      "1600/1600 [==============================] - 12s 7ms/step - loss: 1.3036 - val_loss: 1.6418\n",
      "Result of 560-th epoch:\n",
      "^@mAgytg?t: Trudgels levels belass the moger out... Womper shory in the @foxandaghect worly, at all didn?t will our! Many a amo?s hard to entoruble!??$\n",
      "\n",
      "\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/20\n",
      "1536/1600 [===========================>..] - ETA: 0s - loss: 1.3014"
     ]
    }
   ],
   "source": [
    "#12,13\n",
    "for i in range(400//epochs_per_iteration):\n",
    "    model.fit(X,y, epochs=20, batch_size=128, validation_split=0.2)\n",
    "    epoch += epochs_per_iteration\n",
    "    print(f'Result of {epoch}-th epoch:')\n",
    "    print(generate_tweet())\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Core 3 - Exercise.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
